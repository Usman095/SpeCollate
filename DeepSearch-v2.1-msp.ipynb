{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import queue\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "import csv\n",
    "import re\n",
    "from heapq import merge\n",
    "from sklearn import preprocessing\n",
    "from operator import itemgetter\n",
    "import bisect\n",
    "import pickle\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "import ast\n",
    "import sys\n",
    "\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from IPython.display import display\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from src.snapprocess import simulatespectra as sim\n",
    "from src.snapprocess import process\n",
    "from src.snaputils import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary config func. Original one in the project.\n",
    "class config:\n",
    "    \"\"\"Define constants\"\"\"\n",
    "    AAMass = {'A': 71.037114, 'C': 103.009185, 'D': 115.026943, 'E': 129.042593, 'F': 147.068414, 'G': 57.021464,\n",
    "              'H': 137.058912, 'I': 113.084064, 'K': 128.094963, 'L': 113.084064, 'M': 131.040485, 'N': 114.042927,\n",
    "              'P': 97.052764, 'Q': 128.058578, 'R': 156.101111, 'S': 87.032028, 'T': 101.047679, 'V': 99.068414,\n",
    "              'W': 186.079313, 'Y': 163.0633}\n",
    "\n",
    "    H2O = 18.015\n",
    "    NH3 = 17.031\n",
    "    PROTON = 1.00727647\n",
    "    DEFAULT_PARAM_PATH = os.path.join(os.getcwd(), 'config.ini')\n",
    "    PARAM_PATH = None\n",
    "    l_config = None\n",
    "\n",
    "\n",
    "    def get_config(section='input', key=None):\n",
    "        \"\"\"Read the configuration parameters and return a dictionary.\"\"\"\n",
    "\n",
    "        # If file path is given use it otherwise use default.\n",
    "        file_path = config.PARAM_PATH if config.PARAM_PATH else config.DEFAULT_PARAM_PATH\n",
    "\n",
    "        # Read config and convert each value to appropriate type.\n",
    "        # Only for the first time.\n",
    "        if not config.l_config:\n",
    "            config.l_config = dict()\n",
    "            config_ = ConfigParser()\n",
    "            assert isinstance(file_path, str)\n",
    "            config_.read(file_path)\n",
    "            for section_ in config_.sections():\n",
    "                config.l_config[section_] = dict()\n",
    "                for key_ in config_[section_]:\n",
    "                    try:\n",
    "                        config.l_config[section_][key_] = ast.literal_eval(config_[section_][key_])\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        config.l_config[section_][key_] = config_[section_][key_]\n",
    "\n",
    "        if section and section in config.l_config:\n",
    "            if key and key in config.l_config[section]:\n",
    "                return config.l_config[section][key]\n",
    "            return config.l_config[section]\n",
    "        return config.l_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "False\n",
      "2048\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(config.get_config(section='input', key='charge'))\n",
    "print(config.get_config(section='input', key='use_mods'))\n",
    "print(config.get_config(section='ml', key='batch_size'))\n",
    "print(config.get_config(section='input', key='num_species'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.spec_size = config.get_config(section='input', key='spec_size')\n",
    "        self.searching = False\n",
    "        \n",
    "        self.linear1_1 = nn.Linear(self.spec_size, 1024)\n",
    "        self.linear1_2 = nn.Linear(1024, 512)\n",
    "        self.linear1_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.linear2_1 = nn.Linear(self.spec_size, 1024)\n",
    "        self.linear2_2 = nn.Linear(1024, 512)\n",
    "        self.linear2_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        #self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        res = []\n",
    "#         x = data[:, 0]\n",
    "#         x = self.linear1_1(x.view(-1, self.spec_size))\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "        #x = self.linear1_2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.linear1_3(x)\n",
    "        #x = F.relu(x)\n",
    "        #if not self.searching:\n",
    "        #    x = F.normalize(x)\n",
    "        #res.append(x)\n",
    "        for i in range(data.shape[1]):\n",
    "            x = data[:, i]\n",
    "            x = self.linear2_1(x.view(-1, self.spec_size))\n",
    "            x = F.relu(x)\n",
    "            #x = torch.tanh(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.linear2_2(x)\n",
    "            x = F.relu(x)\n",
    "            #x = torch.tanh(x)\n",
    "            #x = self.dropout2(x)\n",
    "            #x = self.dropout1(x)\n",
    "            #x = self.linear2_3(x)\n",
    "            #x = F.relu(x)\n",
    "            #if not self.searching:\n",
    "            x = F.normalize(x)\n",
    "            #x = self.linear1_3(x)\n",
    "            #x = F.relu(x)\n",
    "            res.append(x)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand.seed(37)\n",
    "\n",
    "config.AAMass\n",
    "H2O = 18.01528\n",
    "PROTON = 1.00727647\n",
    "\n",
    "specsize = 8000\n",
    "precursor_tolerance = 3.0\n",
    "num_psm = 1\n",
    "charge = 2\n",
    "use_mods = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = config.get_config(section='ml', key='batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (linear1_1): Linear(in_features=8000, out_features=1024, bias=True)\n",
       "  (linear1_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear1_3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (linear2_1): Linear(in_features=8000, out_features=1024, bias=True)\n",
       "  (linear2_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2_3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('models/hard_triplet_98.1%_charge_multi-specie.pt').to(device)\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(loader):\n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        out = torch.Tensor().to(device)\n",
    "        for batch_idx, data in enumerate(loader):\n",
    "            data = data.to(device)    \n",
    "            out = torch.cat((out, model(data)[0]), dim=0)\n",
    "    print(out.size())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readmgfs(folderPath, charge=None):\n",
    "    \n",
    "    mgffiles = [f for f in listdir(folderPath) if isfile(join(folderPath, f)) and f.split('.')[-1] == 'mgf']\n",
    "    assert len(mgffiles) > 0\n",
    "    \n",
    "    spectra = []\n",
    "    masses = []\n",
    "    charges = []\n",
    "    for file in mgffiles:\n",
    "        f = open(join(folderPath, file))\n",
    "        speclines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        if not speclines:\n",
    "            continue\n",
    "            \n",
    "        spec = np.zeros(specsize)\n",
    "        isMass = isCharge = isSpec = False\n",
    "        i = 0\n",
    "        '''Read Headers'''\n",
    "        while True:\n",
    "            line = speclines[i]\n",
    "            i += 1\n",
    "            \n",
    "            splits = line.split('=')\n",
    "            if splits[0].upper() == 'PEPMASS':\n",
    "                masses.append(float(splits[1].split(' ')[0]))\n",
    "                isMass = True\n",
    "                \n",
    "            if isMass and splits[0].upper() == 'CHARGE':\n",
    "                l_charge = int(splits[1][0])\n",
    "                if charge and l_charge != charge:\n",
    "                    del masses[-1]\n",
    "                    isMass = False\n",
    "                    isCharge = False\n",
    "                else:\n",
    "                    charges.append(l_charge)\n",
    "                    isCharge = True\n",
    "                break\n",
    "        \n",
    "        '''Read Spectrum'''\n",
    "        while isMass and isCharge and i < len(speclines):\n",
    "            line = speclines[i]\n",
    "            i += 1\n",
    "            \n",
    "            if line != '\\n' and 'END IONS' not in line.upper():\n",
    "                splits = line.split(' ')\n",
    "                moz, intensity = float(splits[0]), float(splits[1])\n",
    "                spec[round(moz)] += round(intensity)\n",
    "            elif 'END IONS' in line.upper():\n",
    "                break\n",
    "                \n",
    "        if isMass and isCharge:        \n",
    "            spec = np.clip(spec, None, 1000.0)\n",
    "            spec = preprocessing.scale(spec)\n",
    "            spectra.append(spec)\n",
    "        \n",
    "    return spectra, masses, charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readms2(file, charge=None):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    spectra = []\n",
    "    masses = []\n",
    "    charges = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i][:-1]\n",
    "        i += 1\n",
    "        \n",
    "        splits = line.split('\\t')\n",
    "        if splits[0] == 'Z' and (charge is None or float(splits[1]) == charge):\n",
    "            charges.append(float(splits[1]))\n",
    "            masses.append(float(splits[2]))\n",
    "            spec = np.zeros(specsize)\n",
    "            while i < len(lines):\n",
    "                line = lines[i][:-1] #remove the \\n character\n",
    "                i += 1\n",
    "                splits = line.split(' ')\n",
    "                if 'S' in splits[0]:\n",
    "                    break\n",
    "                if 'Z' in splits[0]:\n",
    "                    continue\n",
    "                moz, intensity = float(splits[0]), float(splits[1])\n",
    "                spec[round(moz)] += round(intensity)\n",
    "            \n",
    "            spec = np.clip(spec, None, 1000.0)\n",
    "            spec = preprocessing.scale(spec)\n",
    "            spectra.append(spec)\n",
    "            \n",
    "    return spectra, masses, charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n",
      "max peaks: 932\n",
      "count: 8793\n",
      "max moz: 3483.6824\n"
     ]
    }
   ],
   "source": [
    "msp_file = '/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/msfragger/input_small.splib'\n",
    "pep_list, dataset, label = reader.read_msp(msp_file, 0, decoy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2file = '/disk/raptor/lclhome/mtari008/Proteomics/crux/specs/demo.ms2'\n",
    "\n",
    "#queryspectra, spectramasses, _ = readmgfs(specsPath, charge=2)\n",
    "queryspectra, spectramasses, _ = readms2(ms2file, None)\n",
    "with torch.no_grad():\n",
    "    queryspectraTensor = torch.tensor(queryspectra, dtype=torch.float)[:, None, :]\n",
    "    queryspectra_loader = torch.utils.data.DataLoader(dataset=queryspectraTensor, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    queryspectra_out = runModel(queryspectra_loader)\n",
    "del queryspectraTensor\n",
    "del queryspectra_loader\n",
    "del queryspectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(queryspectra_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastatospectra(lines, start, count, dh):\n",
    "    tspectra = []\n",
    "    masses = []\n",
    "    peps = []\n",
    "    \n",
    "    new = prev = 0\n",
    "    end = min(start+count, len(lines))\n",
    "    for i, line in enumerate(lines[start:end]):\n",
    "        splits = line.split('\\t')\n",
    "        \n",
    "        pep = splits[0]\n",
    "        #print(pep)\n",
    "        peps.append(pep)\n",
    "        spec = sim.get_spectrum(pep)\n",
    "        tspectra.append(preprocessing.scale(spec))\n",
    "        masses.append(float(splits[1]))\n",
    "        \n",
    "        #print(splits[1])\n",
    "        '''Progress Monitor'''\n",
    "        new = int(((i+start)/len(lines)) * 100)\n",
    "        if (new > prev):\n",
    "            dh.update(str(new)+'%')\n",
    "            prev = new\n",
    "            \n",
    "    return tspectra, masses, peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastaToSpectraBatch(filePath, spectra_batch_size):    \n",
    "    f = open(filePath)\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    masses = []\n",
    "    spectra_out = torch.Tensor().to(device)\n",
    "    peps = []\n",
    "    \n",
    "    dh = display('0%', display_id=True)\n",
    "    \n",
    "    start = 0\n",
    "    i = 0\n",
    "    while start < len(lines):\n",
    "        print('Batch: ' + str(i))\n",
    "        i += 1\n",
    "        \n",
    "        print('Generating spectra...')\n",
    "        spectra, l_masses, l_peps = fastatospectra(lines, start, spectra_batch_size, dh)\n",
    "        masses.extend(l_masses)\n",
    "        peps.extend(l_peps)\n",
    "        start = start + spectra_batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print('Converting to tensor...')\n",
    "            '''dtype=torch.float'''\n",
    "            spectra = np.asarray(spectra)\n",
    "            spectraTensor = torch.as_tensor(spectra, dtype=torch.float)[:, None, :]\n",
    "            spectra_loader = torch.utils.data.DataLoader(dataset=spectraTensor, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            print('Running the model...')\n",
    "            model_out = runModel(spectra_loader)\n",
    "            spectra_out = torch.cat((spectra_out, model_out), dim=0)\n",
    "            \n",
    "    return spectra_out, masses, peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_batch_size = 500000\n",
    "#targetPath = '/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/rat/peptide-dbs/rat-peptides.target.txt'\n",
    "#decoyPath = '/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/rat/peptide-dbs/rat-peptides.decoy.txt'\n",
    "targetPath = '/disk/raptor/lclhome/mtari008/Proteomics/crux/crux-output/generate-peptides.target.txt'\n",
    "decoyPath = '/disk/raptor/lclhome/mtari008/Proteomics/crux/crux-output/generate-peptides.decoy.txt'\n",
    "targetspectra_out, targetmasses, targetpeptides = fastaToSpectraBatch(targetPath, spectra_batch_size)\n",
    "decoyspectra_out, decoymasses, decoypeptides = fastaToSpectraBatch(decoyPath, spectra_batch_size)\n",
    "\n",
    "\n",
    "print(len(targetspectra_out), len(targetmasses))\n",
    "print(len(decoyspectra_out), len(decoymasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_target_distances, nearest_target_indexes = process.pairwise_distances(\n",
    "    queryspectra_out, targetspectra_out).min(1)\n",
    "\n",
    "nearest_decoy_distances, nearest_decoy_indexes  = process.pairwise_distances(\n",
    "    queryspectra_out,  decoyspectra_out).min(1)\n",
    "\n",
    "nearest_distances, target_decoy = torch.cat((nearest_target_distances.view(1, -1), \n",
    "                                             nearest_decoy_distances.view(1, -1)), dim=0).min(0)\n",
    "\n",
    "print(target_decoy.to('cpu'))\n",
    "\n",
    "sorted_nearest_distances, snd_index = nearest_distances.sort()\n",
    "target_decoy = target_decoy[snd_index]\n",
    "\n",
    "print(target_decoy)\n",
    "\n",
    "## To lookup a peptide corresponding to a value in sorted_nearest_distances at index i:\n",
    "## td = target_deocy[i]            # a binary array. 0: target, 1: decoy\n",
    "## query_index = snd_index[i]        # lookup the index of query spectrum\n",
    "## pep_index = nearest_decoy_indexes[query_index] if td else nearest_target_indexes[query_index]\n",
    "## td_peptide = decoy_peptides[pep_index] if td else target_peptides[pep_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr(td, target_fdr):\n",
    "    target_count = 0\n",
    "    decoy_count = 0\n",
    "    q_values = [0] * len(td)\n",
    "    for idx, val in enumerate(td):\n",
    "        if val == 0:\n",
    "            target_count += 1\n",
    "        else:\n",
    "            decoy_count += 1\n",
    "        return_fdr = decoy_count / (idx + 1)\n",
    "        print('{} / {} = {}'.format(decoy_count, idx+1, decoy_count/(idx+1)))\n",
    "        #if return_fdr > target_fdr:\n",
    "        #    return target_count, return_fdr\n",
    "    return target_count, return_fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdr(target_decoy, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targetmasses[0:10])\n",
    "print(len(decoypeptides))\n",
    "targetspectra_out, targetmasses, targetpeptides = list(zip(*sorted(zip(targetspectra_out, targetmasses, targetpeptides), \n",
    "                                                                   key=lambda pair: pair[1])))\n",
    "decoyspectra_out, decoymasses, decoypeptides = list(zip(*sorted(zip(decoyspectra_out, decoymasses, decoypeptides),\n",
    "                                                                key=lambda pair: pair[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(targetmasses))\n",
    "print(targetmasses[1000:1010])\n",
    "print(decoymasses[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_range(arr, val, tolerance=3):\n",
    "    assert len(arr) > 0\n",
    "    assert tolerance > 0\n",
    "    assert val > 0\n",
    "    \n",
    "    left = val - tolerance if val - tolerance >= 0 else 0\n",
    "    right = val + tolerance\n",
    "    \n",
    "    ileft = bisect.bisect_left(arr, left)\n",
    "    iright = bisect.bisect_right(arr, right)\n",
    "    \n",
    "    return (ileft, iright-1) if iright - ileft > 0 else (-1, -1)\n",
    "    \n",
    "# Function to insert element \n",
    "def insert(lst, n): \n",
    "    assert not lst or len(lst[0]) == len(n)\n",
    "    \n",
    "    index = num_psm\n",
    "    # Searching for the position \n",
    "    for i in range(len(lst)): \n",
    "        if lst[i][0] > n[0]: \n",
    "            index = i \n",
    "            break\n",
    "      \n",
    "    # Inserting n in the list \n",
    "    lst = lst[:index] + [n] + lst[index:] \n",
    "    return lst[:num_psm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_L2_Dist(queryspectra, querymasses, dbspectra, dbmasses, pre_tol=3, keep_psm=1):\n",
    "    prev = 0\n",
    "    psm_scores = []\n",
    "    psm_pepids = []\n",
    "    \n",
    "    for ind, (spec, spec_mass) in enumerate(zip(queryspectra, querymasses)):\n",
    "        left, right = find_range(dbmasses, spec_mass, pre_tol)\n",
    "        if left != -1 and right != -1:\n",
    "            candidate_specs = dbspectra[left:right+1]\n",
    "            l_scores = np.linalg.norm(np.subtract(candidate_specs, spec), axis=1)**2\n",
    "            psm_pepids.append(left + l_scores.argsort()[:keep_psm]) #add argsort later\n",
    "            psm_scores.append(np.sort(l_scores)[:keep_psm])\n",
    "            #print(ind)\n",
    "            new = int((ind/len(queryspectra)) * 100)\n",
    "            if new > prev:\n",
    "                clear_output(wait=True)\n",
    "                print(str(new) + '%')\n",
    "                prev = new\n",
    "        else:\n",
    "            print('***********error**************')\n",
    "            print('ind: ' + str(ind))\n",
    "            print('spec_mass: ' + str(spec_mass))\n",
    "            print('**********end error************')\n",
    "            \n",
    "    return psm_scores, psm_pepids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_xcorr(queryspectra, querymasses, dbspectra, dbmasses, pre_tol=3, keep_psm=1):\n",
    "    prev = 0\n",
    "    psm_scores = []\n",
    "    psm_pepids = []\n",
    "    \n",
    "    for ind, (spec, spec_mass) in enumerate(zip(queryspectra, querymasses)):\n",
    "        left, right = find_range(dbmasses, spec_mass, pre_tol)\n",
    "        if left != -1 and right != -1:\n",
    "            candidate_specs = dbspectra[left:right+1]\n",
    "            l_scores = np.dot(candidate_specs, spec)\n",
    "            psm_pepids.append(left + l_scores.argsort()[::-1][:keep_psm]) #add argsort later\n",
    "            psm_scores.append(np.sort(l_scores)[::-1][:keep_psm])\n",
    "            #print(ind)\n",
    "            new = int((ind/len(queryspectra)) * 100)\n",
    "            if new > prev:\n",
    "                clear_output(wait=True)\n",
    "                print(str(new) + '%')\n",
    "                prev = new\n",
    "        else:\n",
    "            print('***********error**************')\n",
    "            print('ind: ' + str(ind))\n",
    "            print('spec_mass: ' + str(spec_mass))\n",
    "            print('**********end error************')\n",
    "            \n",
    "    return psm_scores, psm_pepids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetpsm_scores, targetpsm_ids = calculate_L2_Dist(queryspectra_out, spectramasses, targetspectra_out,\n",
    "#                                      targetmasses, precursor_tolerance, num_psm)\n",
    "# decoypsm_scores, decoypsm_ids = calculate_L2_Dist(queryspectra_out, spectramasses, decoyspectra_out,\n",
    "#                                      decoymasses, precursor_tolerance, num_psm)\n",
    "targetpsm_scores, targetpsm_ids = calculate_xcorr(queryspectra_out, spectramasses, targetspectra_out,\n",
    "                                     targetmasses, precursor_tolerance, num_psm)\n",
    "decoypsm_scores, decoypsm_ids = calculate_xcorr(queryspectra_out, spectramasses, decoyspectra_out,\n",
    "                                     decoymasses, precursor_tolerance, num_psm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(targetmasses[2000000:2000010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(targetspectra_out))\n",
    "print(len(decoyspectra_out))\n",
    "\n",
    "print(len(targetpsm_scores))\n",
    "print(len(decoypsm_scores))\n",
    "\n",
    "print(targetpsm_scores[0:10])\n",
    "print(decoypsm_scores[0:10])\n",
    "\n",
    "targetpsm_scores = np.asarray(targetpsm_scores).flatten()\n",
    "decoypsm_scores = np.asarray(decoypsm_scores).flatten()\n",
    "\n",
    "targetpsm_ids = np.asarray(targetpsm_ids).flatten()\n",
    "decoypsm_ids = np.asarray(decoypsm_ids).flatten()\n",
    "\n",
    "targetpsm_peps = np.asarray(targetpeptides)[targetpsm_ids]\n",
    "decoypsm_peps = np.asarray(decoypeptides)[decoypsm_ids]\n",
    "\n",
    "#targetpsm_ids = targetpsm_ids[np.argsort(np.asarray(targetpsm_scores).flatten())]\n",
    "#decoypsm_ids = decoypsm_ids[np.argsort(np.asarray(decoypsm_scores).flatten())]\n",
    "\n",
    "sorted_targets = np.sort(np.asarray(targetpsm_scores).flatten())[::-1]\n",
    "sorted_decoys = np.sort(np.asarray(decoypsm_scores).flatten())[::-1]\n",
    "\n",
    "print(targetpsm_ids[0:10])\n",
    "print(decoypsm_ids[0:10])\n",
    "\n",
    "print(sorted_targets[0:100])\n",
    "print(sorted_decoys[0:100])\n",
    "\n",
    "#targetpsm_scores = np.column_stack((targetpsm_scores, np.ones(len(targetpsm_scores))))\n",
    "#decoypsm_scores = np.column_stack((decoypsm_scores, np.zeros(len(decoypsm_scores))))\n",
    "#decoypsm_scores = np.asarray(decoypsm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetpsm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={'spectrum_ids':np.arange(len(targetpsm_peps)), 'target_psms':targetpsm_peps, \n",
    "          'targetpsm_scores':targetpsm_scores, 'decoy_psms':decoypsm_peps, 'decoypsm_scores':decoypsm_scores})\n",
    "df.to_csv('deepsearch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedpeps = np.array(sortedpeps)\n",
    "correct_matches = 0\n",
    "for ind, psms in enumerate(psm_scores):\n",
    "    #print('ind: ' + str(ind) + ', mass: ' + str(sortedspecmasses[ind]))\n",
    "    #print('psms: ' + str(psms) + ', psm mass: ' + str(sortedpepmasses[psms[0]]))\n",
    "    cand_peps = sortedpeps[psms]\n",
    "    #print(cand_peps)\n",
    "    for pep in cand_peps:\n",
    "        #print('pepidmass: ' + str(pepidmass[pep][0]))\n",
    "        #print('ids: ' + str(ids[ind]) + ', in: ' + str(pepidmass[pep][0]))\n",
    "        if ids[ind] in pepidmass[pep][0]:\n",
    "            correct_matches += 1\n",
    "            break\n",
    "print(len(psm_scores))\n",
    "print(correct_matches)\n",
    "print(str((correct_matches/len(psm_scores)) * 100) + '% correct matches in top ' + str(num_psm) + ' psms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5184 - 1739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/rat/peptide-dbs/rat-peptides.target.txt')\n",
    "tlines = file.readlines()\n",
    "print(len(tlines))\n",
    "tpeps = list((tline.split('\\t')[0], float(tline.split('\\t')[1])) for tline in tlines)\n",
    "tpeps = sorted(tpeps, key=lambda pair: pair[1])\n",
    "print(tpeps[10000:10010])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/rat/peptide-dbs/rat-peptides.decoy.txt')\n",
    "tlines = file.readlines()\n",
    "print(len(tlines))\n",
    "tpeps = list((tline.split('\\t')[0], float(tline.split('\\t')[1])) for tline in tlines)\n",
    "tpeps = sorted(tpeps, key=lambda pair: pair[1])\n",
    "print(tpeps[10000:10010])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 3)\n",
    "z = (torch.ones(3) * 5).view(1, 3)\n",
    "print(x)\n",
    "print(z)\n",
    "# or this\n",
    "#z = torch.ones(2, 3, 1) * 5\n",
    "y = torch.cat((x, z), dim=0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep this cell....\n",
    "## How to sort a 2D tensor by its first row.....\n",
    "x = torch.Tensor([5., 3., 4., 1., 2.]).view(1, -1)\n",
    "z = torch.Tensor([1., 3., 2., 5., 4.]).view(1, -1)\n",
    "print(x)\n",
    "print(z)\n",
    "# or this\n",
    "#z = torch.ones(2, 3, 1) * 5\n",
    "y = torch.cat((x, z), dim=0)\n",
    "_, sorted_index_y = y[0, :].sort()\n",
    "sorted_y = y[:, sorted_index_y]\n",
    "print(y)\n",
    "print(sorted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"Name\\:\\s(?P<pep>[a-zA-Z]+)\\/(?P<charge>\\d+)(?:\\_(?P<num_mods>\\d+)(?P<mods>.*))?\"\n",
    "test_str = \"Name: AAAAAEEGMEPR/2\"\n",
    "m = re.search(regex, test_str)\n",
    "print((m['pep']))\n",
    "print(type(int(m.group('charge'))))\n",
    "\n",
    "print(bool(m['pep']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = float(re.findall(r\"MW\\:\\s([-+]?[0-9]*\\.?[0-9]*)\", \"MW: 1261.5610\")[0])\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
