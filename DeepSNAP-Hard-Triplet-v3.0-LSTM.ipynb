{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import math\n",
    "import queue\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "import csv\n",
    "from heapq import merge\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "from configparser import ConfigParser\n",
    "import ast\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "rand.seed(37)\n",
    "\n",
    "#from src.snapconfig import config\n",
    "from src.snaputils import simulatespectra as sim\n",
    "from src.snapprocess import process\n",
    "from src.snaputils import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary config func. Original one in the project.\n",
    "class config:\n",
    "    \"\"\"Define constants\"\"\"\n",
    "    AAMass = {'A': 71.037114, 'C': 103.009185, 'D': 115.026943, 'E': 129.042593, 'F': 147.068414, 'G': 57.021464,\n",
    "              'H': 137.058912, 'I': 113.084064, 'K': 128.094963, 'L': 113.084064, 'M': 131.040485, 'N': 114.042927,\n",
    "              'P': 97.052764, 'Q': 128.058578, 'R': 156.101111, 'S': 87.032028, 'T': 101.047679, 'V': 99.068414,\n",
    "              'W': 186.079313, 'Y': 163.0633}\n",
    "\n",
    "    H2O = 18.015\n",
    "    NH3 = 17.031\n",
    "    PROTON = 1.00727647\n",
    "    DEFAULT_PARAM_PATH = os.path.join(os.getcwd(), 'config.ini')\n",
    "    PARAM_PATH = None\n",
    "    l_config = None\n",
    "\n",
    "\n",
    "    def get_config(section='input', key=None):\n",
    "        \"\"\"Read the configuration parameters and return a dictionary.\"\"\"\n",
    "\n",
    "        # If file path is given use it otherwise use default.\n",
    "        file_path = config.PARAM_PATH if config.PARAM_PATH else config.DEFAULT_PARAM_PATH\n",
    "\n",
    "        # Read config and convert each value to appropriate type.\n",
    "        # Only for the first time.\n",
    "        if not config.l_config:\n",
    "            config.l_config = dict()\n",
    "            config_ = ConfigParser()\n",
    "            assert isinstance(file_path, str)\n",
    "            config_.read(file_path)\n",
    "            for section_ in config_.sections():\n",
    "                config.l_config[section_] = dict()\n",
    "                for key_ in config_[section_]:\n",
    "                    try:\n",
    "                        config.l_config[section_][key_] = ast.literal_eval(config_[section_][key_])\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        config.l_config[section_][key_] = config_[section_][key_]\n",
    "\n",
    "        if section and section in config.l_config:\n",
    "            if key and key in config.l_config[section]:\n",
    "                return config.l_config[section][key]\n",
    "            return config.l_config[section]\n",
    "        return config.l_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_msps(msp_dir, out_dir):\n",
    "    in_path = Path(msp_dir)\n",
    "    assert in_path.exists() and in_path.is_dir()\n",
    "    \n",
    "    msp_files = [join(msp_dir, f) for f in listdir(msp_dir) if\n",
    "                 isfile(join(msp_dir, f)) and f.split('.')[-1] == 'msp']\n",
    "    assert len(msp_files) > 0\n",
    "    \n",
    "    out_path = Path(out_dir)\n",
    "    if out_path.exists() and out_path.is_dir():\n",
    "        shutil.rmtree(out_path)\n",
    "    out_path.mkdir()\n",
    "    Path(join(out_path, 'spectra')).mkdir()\n",
    "    Path(join(out_path, 'peptides')).mkdir()\n",
    "        \n",
    "    print('reading {} files'.format(len(msp_files)))\n",
    "    \n",
    "    count = 0\n",
    "    max_peaks = max_moz = 0\n",
    "    for species_id, msp_file in enumerate(msp_files):\n",
    "        print('Reading: {}'.format(msp_file))\n",
    "        \n",
    "        f = open(msp_file, \"r\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        pep_list = []\n",
    "        dataset = []\n",
    "        label = []\n",
    "\n",
    "        # FIXME: config should use only one get_config call.\n",
    "        spec_size = config.get_config(section='input', key='spec_size')\n",
    "        charge = config.get_config(section='input', key='charge')\n",
    "        use_mods = config.get_config(section='input', key='use_mods')\n",
    "        num_species = config.get_config(section='input', key='num_species')\n",
    "        seq_len = config.get_config(section='ml', key='pep_seq_len')\n",
    "\n",
    "        print('len of file: ' + str(len(lines)))\n",
    "        limit = 200000\n",
    "        pep = []\n",
    "        spec = []\n",
    "        pep_set = set()\n",
    "        is_name = is_mw = is_num_peaks = False\n",
    "        prev = 0\n",
    "        i = 0\n",
    "        while i < len(lines) and limit > 0:\n",
    "            line = lines[i]\n",
    "            i += 1\n",
    "            if line.startswith('Name:'):\n",
    "                name_groups = re.search(r\"Name:\\s(?P<pep>[a-zA-Z]+)/(?P<charge>\\d+)\"\n",
    "                                        r\"(?:_(?P<num_mods>\\d+)(?P<mods>.*))?\", line)\n",
    "                if not name_groups:\n",
    "                    continue\n",
    "                    \n",
    "                pep = name_groups['pep']\n",
    "                if len(pep) + 1 > seq_len:\n",
    "                    continue\n",
    "                    \n",
    "                l_charge = int(name_groups['charge'])\n",
    "                num_mods = int(name_groups['num_mods'])\n",
    "\n",
    "                is_name = True\n",
    "\n",
    "            if is_name and line.startswith('MW:'):\n",
    "                mass = float(re.findall(r\"MW:\\s([-+]?[0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                if round(mass) < spec_size:\n",
    "                    is_mw = True\n",
    "                    # limit = limit - 1\n",
    "                else:\n",
    "                    is_name = is_mw = is_num_peaks = False\n",
    "                    continue\n",
    "\n",
    "            if is_name and is_mw and line.startswith('Num peaks:'):\n",
    "                num_peaks = int(re.findall(r\"Num peaks:\\s([0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                if num_peaks > max_peaks:\n",
    "                    max_peaks = num_peaks\n",
    "\n",
    "                spec = np.zeros(spec_size)\n",
    "                while lines[i] != '\\n':\n",
    "                    mz_line = lines[i]\n",
    "                    i += 1\n",
    "                    mz_splits = mz_line.split('\\t')\n",
    "                    moz, intensity = float(mz_splits[0]), float(mz_splits[1])\n",
    "                    if moz > max_moz:\n",
    "                        max_moz = moz\n",
    "                    spec[round(moz)] += round(intensity)\n",
    "\n",
    "                # for k in range(1, charge + 1):\n",
    "                #     spec[-k] = 0\n",
    "                # spec[-l_charge] = 1000.0\n",
    "                spec = np.clip(spec, None, 1000.0)\n",
    "                # spec = preprocessing.scale(spec)\n",
    "\n",
    "                is_num_peaks = True\n",
    "\n",
    "            if is_name and is_mw and is_num_peaks:\n",
    "                is_name = is_mw = is_num_peaks = False\n",
    "                \n",
    "                #pep = '{}{}{}'.format(charge, species_id, pep)\n",
    "\n",
    "                \"\"\"output the data to \"\"\"\n",
    "                spec_tensor = torch.tensor((np.asarray(spec) - 3.725) / 51.479, dtype=torch.float)\n",
    "                \n",
    "                torch.save(spec_tensor, \n",
    "                           join(out_dir, 'spectra', '{}-{}-{}-{}-{}.pt'\n",
    "                                .format(count, species_id, mass, l_charge, int(num_mods > 0))))\n",
    "                \n",
    "                pep_file_name = '{}-{}-{}-{}-{}.pep'.format(count, species_id, mass, l_charge, int(num_mods > 0))\n",
    "                    \n",
    "                with open(join(out_path, 'peptides', pep_file_name), 'w+') as f:\n",
    "                    f.write(pep)\n",
    "\n",
    "                count = count + 1\n",
    "                pep = 0\n",
    "                spec = []\n",
    "                new = int((i / len(lines)) * 100)\n",
    "                if new > prev:\n",
    "                    clear_output(wait=True)\n",
    "                    print(str(new) + '%')\n",
    "                    prev = new\n",
    "\n",
    "        print('max peaks: ' + str(max_peaks))\n",
    "        print('count: ' + str(count))\n",
    "        print('max moz: ' + str(max_moz))\n",
    "#         return pep_list, dataset, label\n",
    "#         tmp_pep_list, tmp_dataset, tmp_labels = read_msp(msp_file, species_id, decoy)\n",
    "#         pep_list.extend(tmp_dataset)\n",
    "#         dataset.extend(tmp_dataset)\n",
    "#         label.extend(tmp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_mgfs_back(mgf_dir, out_dir):\n",
    "    in_path = Path(mgf_dir)\n",
    "    assert in_path.exists() and in_path.is_dir()\n",
    "    \n",
    "    mgf_files = [join(mgf_dir, f) for f in listdir(mgf_dir) if\n",
    "                 isfile(join(mgf_dir, f)) and f.split('.')[-1] == 'mgf']\n",
    "    assert len(mgf_files) > 0\n",
    "    \n",
    "    out_path = Path(out_dir)\n",
    "    if out_path.exists() and out_path.is_dir():\n",
    "        shutil.rmtree(out_path)\n",
    "        \n",
    "    out_path.mkdir()\n",
    "    Path(join(out_path, 'spectra')).mkdir()\n",
    "    Path(join(out_path, 'peptides')).mkdir()\n",
    "        \n",
    "    print('reading {} files'.format(len(mgf_files)))\n",
    "    \n",
    "    ch = np.zeros(20)\n",
    "    modified = 0\n",
    "    unmodified = 0\n",
    "    unique_pep_set = set()\n",
    "    \n",
    "    summ = 0\n",
    "    sq_sum = 0\n",
    "    N = 0\n",
    "    \n",
    "    tot_count = 0\n",
    "    max_peaks = max_moz = 0\n",
    "    for species_id, mgf_file in enumerate(mgf_files):\n",
    "        print('Reading: {}'.format(mgf_file))\n",
    "        \n",
    "        f = open(mgf_file, \"r\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        count = lcount = 0\n",
    "\n",
    "        pep_list = []\n",
    "        dataset = []\n",
    "        label = []\n",
    "        \n",
    "        mass_ign = 0\n",
    "        pep_len_ign = 0\n",
    "        dup_ign = 0\n",
    "\n",
    "        # FIXME: config should use only one get_config call.\n",
    "        spec_size = config.get_config(section='input', key='spec_size')\n",
    "        charge = config.get_config(section='input', key='charge')\n",
    "        use_mods = config.get_config(section='input', key='use_mods')\n",
    "        num_species = config.get_config(section='input', key='num_species')\n",
    "        seq_len = config.get_config(section='ml', key='pep_seq_len')\n",
    "\n",
    "        print('len of file: ' + str(len(lines)))\n",
    "        limit = 200000\n",
    "        pep = []\n",
    "        spec = []\n",
    "        pep_set = set()\n",
    "        is_name = is_mw = is_charge = False\n",
    "        prev = 0\n",
    "        i = 0\n",
    "        while i < len(lines) and limit > 0:\n",
    "            line = lines[i]\n",
    "            i += 1\n",
    "\n",
    "            if line.startswith('PEPMASS'):\n",
    "                count += 1\n",
    "                mass = float(re.findall(r\"PEPMASS=([-+]?[0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                if round(mass) < spec_size:\n",
    "                    is_mw = True\n",
    "                    # limit = limit - 1\n",
    "                else:\n",
    "                    is_name = is_mw = is_charge = False\n",
    "                    mass_ign += 1\n",
    "                    continue\n",
    "            \n",
    "            if is_mw and line.startswith('CHARGE'):\n",
    "                l_charge = int(re.findall(r\"CHARGE=([-+]?[0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                is_charge = True\n",
    "                \n",
    "            if is_mw and is_charge and line.startswith(\"SEQ\"):\n",
    "                line = line.strip()\n",
    "                pep_segs = re.findall(r\"([A-Z]+)\", line)[1:]\n",
    "                num_mods = len(pep_segs) - 1 + (1 if line[-1] == ')' else 0)\n",
    "                pep = ''.join(pep_segs)\n",
    "                mods = re.findall(r\"([A-Z]\\([^\\(\\)]*\\))\", line)\n",
    "                \n",
    "                if len(pep) + 2 > seq_len:\n",
    "                    pep_len_ign += 1\n",
    "                    is_name = is_mw = is_charge = False\n",
    "                    continue\n",
    "                \n",
    "                rtinsec = lines[i-2]\n",
    "                rt = round(float(re.findall(r\"RTINSECONDS=([-+]?[0-9]*\\.?[0-9]*)\", rtinsec)[0]))\n",
    "                check_pep = line[4:] + str(l_charge) + \"-\" + str(rt)\n",
    "                # check_pep = pep + str(l_charge)\n",
    "                if check_pep in pep_set:\n",
    "                    dup_ign += 1\n",
    "                    is_mw = is_charge = is_name = False\n",
    "                    continue\n",
    "                else:\n",
    "                    ch[l_charge] += 1\n",
    "                    if num_mods > 0:\n",
    "                        modified += 1\n",
    "                    else:\n",
    "                        unmodified += 1\n",
    "                    if pep not in unique_pep_set:\n",
    "                        unique_pep_set.add(pep)\n",
    "                    pep_set.add(check_pep)\n",
    "                    \n",
    "\n",
    "                ind = [] # setting the precision to one decimal point.\n",
    "                val = []\n",
    "                #for ch_val in range(l_charge):\n",
    "                #    ind.append(ch_val)\n",
    "                #    val.append(1)\n",
    "                    \n",
    "                while 'END IONS' not in lines[i].upper():\n",
    "                    if lines[i] == '\\n':\n",
    "                        continue\n",
    "                    mz_line = lines[i]\n",
    "                    i += 1\n",
    "                    mz_splits = mz_line.split(' ')\n",
    "                    moz, intensity = float(mz_splits[0]), float(mz_splits[1])\n",
    "                    if moz > max_moz:\n",
    "                        max_moz = moz\n",
    "                    if 0 < round(moz) < spec_size:\n",
    "                        # spec[round(moz*10)] += round(intensity)\n",
    "                        ind.append(round(moz*10))\n",
    "                        val.append(intensity)\n",
    "                        \n",
    "                        \n",
    "                # for k in range(1, charge + 1):\n",
    "                #     spec[-k] = 0\n",
    "                # spec[-l_charge] = 1000.0\n",
    "                \n",
    "                ind = np.array(ind)\n",
    "                val = np.clip(val, None, 10000.0)\n",
    "                assert len(ind) == len(val)\n",
    "                spec = np.array([ind, val])\n",
    "                \n",
    "                summ += sum(val)\n",
    "                sq_sum += sum(s_val*s_val for s_val in val)\n",
    "                N += 80000\n",
    "                # spec = preprocessing.scale(spec)\n",
    "\n",
    "                is_name = True\n",
    "\n",
    "            if is_name and is_mw and is_charge:\n",
    "                is_name = is_mw = is_charge = False\n",
    "                \n",
    "                #pep = '{}{}{}'.format(charge, species_id, pep)\n",
    "\n",
    "                \"\"\"output the data to \"\"\"\n",
    "                #spec_tensor = torch.tensor((np.asarray(spec) - 3.725) / 51.479, dtype=torch.float)\n",
    "                # ind = torch.LongTensor([[0]*len(ind), ind])\n",
    "                # val = torch.FloatTensor(val)\n",
    "                # spec_tensor = torch.sparse.FloatTensor(ind, val, torch.Size([1, 80000]))\n",
    "                \n",
    "                # spec_tensor = torch.tensor(np.asarray(spec), dtype=torch.float)\n",
    "                \n",
    "#                 torch.save(spec_tensor, \n",
    "#                            join(out_dir, 'spectra', '{}-{}-{}-{}-{}.pt'\n",
    "#                                 .format(tot_count, species_id, mass, l_charge, int(num_mods > 0))))\n",
    "                \n",
    "                np.save(join(out_dir, 'spectra', '{}-{}-{}-{}-{}.npy'\n",
    "                             .format(tot_count, species_id, mass, l_charge, int(num_mods > 0))),\n",
    "                       spec)\n",
    "                \n",
    "                pep_file_name = '{}-{}-{}-{}-{}.pep'.format(tot_count, species_id, mass, l_charge, int(num_mods > 0))\n",
    "                    \n",
    "                with open(join(out_dir, 'peptides', pep_file_name), 'w+') as f:\n",
    "                    f.write(pep)\n",
    "\n",
    "                lcount += 1\n",
    "                tot_count += 1\n",
    "                \n",
    "                pep = 0\n",
    "                spec = []\n",
    "                new = int((i / len(lines)) * 100)\n",
    "                if new >= prev + 10:\n",
    "                    #clear_output(wait=True)\n",
    "                    print('count: ' + str(lcount))\n",
    "                    print(str(new) + '%')\n",
    "                    prev = new\n",
    "\n",
    "        #print('max peaks: ' + str(max_peaks))\n",
    "        print('In current file, read {} out of {}'.format(lcount, count))\n",
    "        print(\"Ignored: large mass: {}, pep len: {}, dup: {}\".format(mass_ign, pep_len_ign, dup_ign))\n",
    "        print('overall running count: ' + str(tot_count))\n",
    "        print('max moz: ' + str(max_moz))\n",
    "#         return pep_list, dataset, label\n",
    "#         tmp_pep_list, tmp_dataset, tmp_labels = read_msp(msp_file, species_id, decoy)\n",
    "#         pep_list.extend(tmp_dataset)\n",
    "#         dataset.extend(tmp_dataset)\n",
    "#         label.extend(tmp_labels)\n",
    "    print(\"Statistics:\")\n",
    "    print(\"Charge distribution:\")\n",
    "    print(ch)\n",
    "    print(\"Modified:\\t{}\".format(modified))\n",
    "    print(\"Unmodified:\\t{}\".format(unmodified))\n",
    "    print(\"Unique Peptides:\\t{}\".format(len(unique_pep_set)))\n",
    "    print(\"Sum: {}\".format(summ))\n",
    "    print(\"Sum-Squared: {}\".format(sq_sum))\n",
    "    print(\"N: {}\".format(N))\n",
    "    mean = summ / N\n",
    "    print(\"mean: {}\".format(mean))\n",
    "    std = math.sqrt((sq_sum / N) - mean**2)\n",
    "    print(\"std: {}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_out_dir(dir_path, exist_ok=True):\n",
    "    out_path = Path(dir_path)\n",
    "    if out_path.exists() and out_path.is_dir():\n",
    "        if not exist_ok:\n",
    "            shutil.rmtree(out_path)\n",
    "            out_path.mkdir()\n",
    "    else:\n",
    "        out_path.mkdir()\n",
    "        \n",
    "    Path(join(out_path, 'spectra')).mkdir()\n",
    "    Path(join(out_path, 'peptides')).mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_in_dir(dir_path, ext):\n",
    "    in_path = Path(dir_path)\n",
    "    assert in_path.exists() and in_path.is_dir()\n",
    "    \n",
    "    files = [join(dir_path, f) for f in listdir(dir_path) if\n",
    "                 isfile(join(dir_path, f)) and f.split('.')[-1] == ext]\n",
    "    assert len(files) > 0\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(str_float):\n",
    "    try:\n",
    "        float(str_float)\n",
    "        return True\n",
    "    except ValueError: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mgfs(mgf_dir, out_dir):\n",
    "    \n",
    "    mgf_files = verify_in_dir(mgf_dir, \"mgf\")\n",
    "    create_out_dir(out_dir, exist_ok=False)\n",
    "        \n",
    "    print('reading {} files'.format(len(mgf_files)))\n",
    "    \n",
    "    ch = np.zeros(20)\n",
    "    modified = 0\n",
    "    unmodified = 0\n",
    "    unique_pep_set = set()\n",
    "    \n",
    "    pep_dict = {}\n",
    "    idx_spec_map = []\n",
    "    pep_spec = []\n",
    "    pep_idx = 0\n",
    "    \n",
    "    summ = 0\n",
    "    sq_sum = 0\n",
    "    N = 0\n",
    "    \n",
    "    tot_count = 0\n",
    "    max_peaks = max_moz = 0\n",
    "    for species_id, mgf_file in enumerate(mgf_files):\n",
    "        print('Reading: {}'.format(mgf_file))\n",
    "        \n",
    "        f = open(mgf_file, \"r\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        count = lcount = 0\n",
    "        \n",
    "        pep_list = []\n",
    "        dataset = []\n",
    "        label = []\n",
    "        \n",
    "        mass_ign = 0\n",
    "        pep_len_ign = 0\n",
    "        dup_ign = 0\n",
    "\n",
    "        # FIXME: config should use only one get_config call.\n",
    "        spec_size = config.get_config(section='input', key='spec_size')\n",
    "        charge = config.get_config(section='input', key='charge')\n",
    "        use_mods = config.get_config(section='input', key='use_mods')\n",
    "        num_species = config.get_config(section='input', key='num_species')\n",
    "        seq_len = config.get_config(section='ml', key='pep_seq_len')\n",
    "\n",
    "        print('len of file: ' + str(len(lines)))\n",
    "        limit = 200000\n",
    "        pep = []\n",
    "        spec = []\n",
    "        pep_set = set()\n",
    "        is_name = is_mw = is_charge = False\n",
    "        prev = 0\n",
    "        i = 0\n",
    "        while i < len(lines) and limit > 0:\n",
    "            line = lines[i]\n",
    "            i += 1\n",
    "\n",
    "            if line.startswith('PEPMASS'):\n",
    "                count += 1\n",
    "                mass = float(re.findall(r\"PEPMASS=([-+]?[0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                if round(mass)*10 < spec_size:\n",
    "                    is_mw = True\n",
    "                    # limit = limit - 1\n",
    "                else:\n",
    "                    is_name = is_mw = is_charge = False\n",
    "                    mass_ign += 1\n",
    "                    continue\n",
    "            \n",
    "            if is_mw and line.startswith('CHARGE'):\n",
    "                l_charge = int(re.findall(r\"CHARGE=([-+]?[0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                is_charge = True\n",
    "                \n",
    "            if is_mw and is_charge and line.startswith(\"SEQ\"):\n",
    "                line = line.strip()\n",
    "                pep_segs = re.findall(r\"([A-Z]+)\", line)[1:]\n",
    "                num_mods = len(pep_segs) - 1 + (1 if line[-1] == ')' or line[-1].isdigit() else 0)\n",
    "                pep = ''.join(pep_segs)\n",
    "                #mods = re.findall(r\"([A-Z]\\([^\\(\\)]*\\))\", line)\n",
    "                \n",
    "                if len(pep) + 2 > seq_len:\n",
    "                    pep_len_ign += 1\n",
    "                    is_name = is_mw = is_charge = False\n",
    "                    continue\n",
    "                \n",
    "                # rtinsec = lines[i-2]\n",
    "                # rt = round(float(re.findall(r\"RTINSECONDS=([-+]?[0-9]*\\.?[0-9]*)\", rtinsec)[0]))\n",
    "                check_pep = line[4:] + str(l_charge)#  + \"-\" + str(rt)\n",
    "                # check_pep = pep + str(l_charge)\n",
    "                sub_spec_count = 1\n",
    "                pep_file_name = ''\n",
    "                spec_file_name = ''\n",
    "                \n",
    "                ch[l_charge] += 1\n",
    "                if num_mods > 0:\n",
    "                    modified += 1\n",
    "                else:\n",
    "                    unmodified += 1\n",
    "\n",
    "                if pep not in pep_dict:\n",
    "                    pep_dict[pep] = pep_idx\n",
    "                    idx_spec_map.append(sub_spec_count)\n",
    "                    assert len(idx_spec_map) == len(pep_dict)\n",
    "                    tot_count = pep_idx\n",
    "                    pep_file_name = '{}.pep'.format(tot_count)\n",
    "                    spec_file_name = '{}.{}-{}-{}-{}-{}.npy'.format(\n",
    "                        tot_count, sub_spec_count, species_id, mass, l_charge, int(num_mods > 0))\n",
    "                    pep_spec.append([pep_file_name, [spec_file_name]])\n",
    "                    pep_idx += 1\n",
    "                else:\n",
    "                    tot_count = pep_dict[pep]\n",
    "                    sub_spec_count = idx_spec_map[tot_count] + 1\n",
    "                    idx_spec_map[tot_count] = sub_spec_count\n",
    "                    spec_file_name = '{}.{}-{}-{}-{}-{}.npy'.format(\n",
    "                        tot_count, sub_spec_count, species_id, mass, l_charge, int(num_mods > 0))\n",
    "                    pep_spec[tot_count][1].append(spec_file_name)\n",
    "                    \n",
    "                        \n",
    "                    if pep not in unique_pep_set:\n",
    "                        unique_pep_set.add(pep)\n",
    "                    pep_set.add(check_pep)\n",
    "                    \n",
    "\n",
    "                ind = [] # setting the precision to one decimal point.\n",
    "                val = []\n",
    "                #for ch_val in range(l_charge):\n",
    "                #    ind.append(ch_val)\n",
    "                #    val.append(1)\n",
    "\n",
    "                while not isfloat(re.split(' |\\t|=', lines[i])[0]):\n",
    "                    i += 1\n",
    "                    \n",
    "                while 'END IONS' not in lines[i].upper():\n",
    "                    if lines[i] == '\\n':\n",
    "                        continue\n",
    "                    mz_line = lines[i]\n",
    "                    i += 1\n",
    "                    mz_splits = re.split(' |\\t', mz_line)\n",
    "                    moz, intensity = float(mz_splits[0]), float(mz_splits[1])\n",
    "                    if moz > max_moz:\n",
    "                        max_moz = moz\n",
    "                    if 0 < round(moz*10) < spec_size:\n",
    "                        # spec[round(moz*10)] += round(intensity)\n",
    "                        ind.append(round(moz*10))\n",
    "                        val.append(intensity)\n",
    "                        \n",
    "                        \n",
    "                # for k in range(1, charge + 1):\n",
    "                #     spec[-k] = 0\n",
    "                # spec[-l_charge] = 1000.0\n",
    "                \n",
    "                ind = np.array(ind)\n",
    "                val = np.clip(val, None, 10000.0)\n",
    "                assert len(ind) == len(val)\n",
    "                spec = np.array([ind, val])\n",
    "                \n",
    "                summ += sum(val)\n",
    "                sq_sum += sum(s_val*s_val for s_val in val)\n",
    "                N += 80000\n",
    "                # spec = preprocessing.scale(spec)\n",
    "\n",
    "                is_name = True\n",
    "\n",
    "            if is_name and is_mw and is_charge:\n",
    "                is_name = is_mw = is_charge = False\n",
    "                \n",
    "                #pep = '{}{}{}'.format(charge, species_id, pep)\n",
    "\n",
    "                \"\"\"output the data to \"\"\"\n",
    "                #spec_tensor = torch.tensor((np.asarray(spec) - 3.725) / 51.479, dtype=torch.float)\n",
    "                # ind = torch.LongTensor([[0]*len(ind), ind])\n",
    "                # val = torch.FloatTensor(val)\n",
    "                # spec_tensor = torch.sparse.FloatTensor(ind, val, torch.Size([1, 80000]))\n",
    "                \n",
    "                # spec_tensor = torch.tensor(np.asarray(spec), dtype=torch.float)\n",
    "                \n",
    "#                 torch.save(spec_tensor, \n",
    "#                            join(out_dir, 'spectra', '{}-{}-{}-{}-{}.pt'\n",
    "#                                 .format(tot_count, species_id, mass, l_charge, int(num_mods > 0))))\n",
    "                \n",
    "                np.save(join(out_dir, 'spectra', '{}.{}-{}-{}-{}-{}.npy'\n",
    "                             .format(tot_count, sub_spec_count, species_id, mass, l_charge, int(num_mods > 0))),\n",
    "                       spec)\n",
    "        \n",
    "                # sub_spec_count > 1 means more than one spectra for the same peptide \n",
    "                # therefore, no need to save the peptide again.\n",
    "                if sub_spec_count == 1:\n",
    "                    pep_file_name = '{}.pep'.format(tot_count)\n",
    "                    \n",
    "                    with open(join(out_dir, 'peptides', pep_file_name), 'w+') as f:\n",
    "                        f.write(pep)\n",
    "\n",
    "                lcount += 1\n",
    "                \n",
    "                pep = 0\n",
    "                spec = []\n",
    "                new = int((i / len(lines)) * 100)\n",
    "                if new >= prev + 10:\n",
    "                    #clear_output(wait=True)\n",
    "                    print('count: ' + str(lcount))\n",
    "                    print(str(new) + '%')\n",
    "                    prev = new\n",
    "\n",
    "        #print('max peaks: ' + str(max_peaks))\n",
    "        print('In current file, read {} out of {}'.format(lcount, count))\n",
    "        print(\"Ignored: large mass: {}, pep len: {}, dup: {}\".format(mass_ign, pep_len_ign, dup_ign))\n",
    "        print('overall running count: ' + str(tot_count))\n",
    "        print('max moz: ' + str(max_moz))\n",
    "#         return pep_list, dataset, label\n",
    "#         tmp_pep_list, tmp_dataset, tmp_labels = read_msp(msp_file, species_id, decoy)\n",
    "#         pep_list.extend(tmp_dataset)\n",
    "#         dataset.extend(tmp_dataset)\n",
    "#         label.extend(tmp_labels)\n",
    "\n",
    "    # save the map. this will be used to generate masks for hard positive/negative mining during training.\n",
    "    # np.save(join(out_dir, \"idx_spec_map.npy\"), idx_spec_map)\n",
    "    with open(join(out_dir, 'pep_spec.pkl'), 'wb') as f:\n",
    "        pickle.dump(pep_spec, f)\n",
    "    \n",
    "    print(\"Statistics:\")\n",
    "    print(\"Charge distribution:\")\n",
    "    print(ch)\n",
    "    print(\"Modified:\\t{}\".format(modified))\n",
    "    print(\"Unmodified:\\t{}\".format(unmodified))\n",
    "    print(\"Unique Peptides:\\t{}\".format(len(unique_pep_set)))\n",
    "    print(\"Sum: {}\".format(summ))\n",
    "    print(\"Sum-Squared: {}\".format(sq_sum))\n",
    "    print(\"N: {}\".format(N))\n",
    "    mean = summ / N\n",
    "    print(\"mean: {}\".format(mean))\n",
    "    std = math.sqrt((sq_sum / N) - mean**2)\n",
    "    print(\"std: {}\".format(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msp_dir = \"/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/msp-labeled/\"\n",
    "# in_tensor_dir = \"/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/train_lstm/\"\n",
    "\n",
    "in_dir = \"/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/\"\n",
    "in_tensor_dir = \"/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/train_lstm_hcd_all/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 10 files\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/bacillus.mgf\n",
      "len of file: 57730913\n",
      "count: 30118\n",
      "10%\n",
      "count: 60581\n",
      "20%\n",
      "count: 90536\n",
      "30%\n",
      "count: 119055\n",
      "40%\n",
      "count: 148177\n",
      "50%\n",
      "count: 176550\n",
      "60%\n",
      "count: 206355\n",
      "70%\n",
      "count: 234922\n",
      "80%\n",
      "count: 263738\n",
      "90%\n",
      "In current file, read 291782 out of 291783\n",
      "Ignored: large mass: 0, pep len: 1, dup: 0\n",
      "overall running count: 12709\n",
      "max moz: 29080.447\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/tomato.mgf\n",
      "len of file: 35055857\n",
      "count: 32477\n",
      "10%\n",
      "count: 61450\n",
      "20%\n",
      "count: 87019\n",
      "30%\n",
      "count: 112235\n",
      "40%\n",
      "count: 135620\n",
      "50%\n",
      "count: 162914\n",
      "60%\n",
      "count: 202623\n",
      "70%\n",
      "count: 231178\n",
      "80%\n",
      "count: 255681\n",
      "90%\n",
      "In current file, read 290031 out of 290050\n",
      "Ignored: large mass: 0, pep len: 19, dup: 0\n",
      "overall running count: 85471\n",
      "max moz: 41364.117\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/ricebean.mgf\n",
      "len of file: 7611469\n",
      "count: 3768\n",
      "10%\n",
      "count: 7563\n",
      "20%\n",
      "count: 11265\n",
      "30%\n",
      "count: 14933\n",
      "40%\n",
      "count: 18727\n",
      "50%\n",
      "count: 22476\n",
      "60%\n",
      "count: 26254\n",
      "70%\n",
      "count: 29910\n",
      "80%\n",
      "count: 33726\n",
      "90%\n",
      "In current file, read 37775 out of 37775\n",
      "Ignored: large mass: 0, pep len: 0, dup: 0\n",
      "overall running count: 89242\n",
      "max moz: 41364.117\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/mouse.mgf\n",
      "len of file: 2970102\n",
      "count: 3833\n",
      "10%\n",
      "count: 8055\n",
      "20%\n",
      "count: 11101\n",
      "30%\n",
      "count: 14036\n",
      "40%\n",
      "count: 17497\n",
      "50%\n",
      "count: 21154\n",
      "60%\n",
      "count: 24800\n",
      "70%\n",
      "count: 28258\n",
      "80%\n",
      "count: 32701\n",
      "90%\n",
      "In current file, read 37021 out of 37021\n",
      "Ignored: large mass: 0, pep len: 0, dup: 0\n",
      "overall running count: 110459\n",
      "max moz: 41364.117\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/yeast.mgf\n",
      "len of file: 25411746\n",
      "count: 12805\n",
      "10%\n",
      "count: 23666\n",
      "20%\n",
      "count: 34009\n",
      "30%\n",
      "count: 46513\n",
      "40%\n",
      "count: 57552\n",
      "50%\n",
      "count: 67659\n",
      "60%\n",
      "count: 79591\n",
      "70%\n",
      "count: 90997\n",
      "80%\n",
      "count: 101288\n",
      "90%\n",
      "In current file, read 111301 out of 111312\n",
      "Ignored: large mass: 0, pep len: 11, dup: 0\n",
      "overall running count: 131597\n",
      "max moz: 47568.184\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/masive-all.mgf\n",
      "len of file: 250114903\n",
      "count: 215407\n",
      "10%\n",
      "count: 430302\n",
      "20%\n",
      "count: 645471\n",
      "30%\n",
      "count: 861628\n",
      "40%\n",
      "count: 1076845\n",
      "50%\n",
      "count: 1292170\n",
      "60%\n",
      "count: 1507967\n",
      "70%\n",
      "count: 1723441\n",
      "80%\n",
      "count: 1939124\n",
      "90%\n",
      "In current file, read 2154269 out of 2154269\n",
      "Ignored: large mass: 0, pep len: 0, dup: 0\n",
      "overall running count: 1238405\n",
      "max moz: 47568.184\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/mmazei.mgf\n",
      "len of file: 26697000\n",
      "count: 17528\n",
      "10%\n",
      "count: 34461\n",
      "20%\n",
      "count: 50055\n",
      "30%\n",
      "count: 65545\n",
      "40%\n",
      "count: 80909\n",
      "50%\n",
      "count: 96255\n",
      "60%\n",
      "count: 114662\n",
      "70%\n",
      "count: 130981\n",
      "80%\n",
      "count: 148248\n",
      "90%\n",
      "In current file, read 164421 out of 164421\n",
      "Ignored: large mass: 0, pep len: 0, dup: 0\n",
      "overall running count: 1244161\n",
      "max moz: 47568.184\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/honeybee.mgf\n",
      "len of file: 52737248\n",
      "count: 24244\n",
      "10%\n",
      "count: 49329\n",
      "20%\n",
      "count: 77678\n",
      "30%\n",
      "count: 109655\n",
      "40%\n",
      "count: 147239\n",
      "50%\n",
      "count: 178174\n",
      "60%\n",
      "count: 219221\n",
      "70%\n",
      "count: 265974\n",
      "80%\n",
      "count: 290830\n",
      "90%\n",
      "In current file, read 314550 out of 314571\n",
      "Ignored: large mass: 0, pep len: 21, dup: 0\n",
      "overall running count: 1278513\n",
      "max moz: 47568.184\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/clambacteria.mgf\n",
      "len of file: 14272272\n",
      "count: 14045\n",
      "10%\n",
      "count: 27827\n",
      "20%\n",
      "count: 43919\n",
      "30%\n",
      "count: 58642\n",
      "40%\n",
      "count: 73485\n",
      "50%\n",
      "count: 88878\n",
      "60%\n",
      "count: 100119\n",
      "70%\n",
      "count: 116326\n",
      "80%\n",
      "count: 131978\n",
      "90%\n",
      "In current file, read 150596 out of 150611\n",
      "Ignored: large mass: 0, pep len: 15, dup: 0\n",
      "overall running count: 1311154\n",
      "max moz: 47568.184\n",
      "Reading: /disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/mgfs-hcd/human.mgf\n",
      "len of file: 16020274\n",
      "count: 20329\n",
      "10%\n",
      "count: 36457\n",
      "20%\n",
      "count: 53894\n",
      "30%\n",
      "count: 70824\n",
      "40%\n",
      "count: 83388\n",
      "50%\n",
      "count: 92414\n",
      "60%\n",
      "count: 102025\n",
      "70%\n",
      "count: 111217\n",
      "80%\n",
      "count: 120406\n",
      "90%\n",
      "In current file, read 130583 out of 130583\n",
      "Ignored: large mass: 0, pep len: 0, dup: 0\n",
      "overall running count: 1332830\n",
      "max moz: 47568.184\n",
      "Statistics:\n",
      "Charge distribution:\n",
      "[0.000000e+00 1.366900e+04 1.860282e+06 1.318559e+06 3.823000e+05\n",
      " 8.672200e+04 1.765100e+04 2.688000e+03 4.580000e+02 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      "Modified:\t1064400\n",
      "Unmodified:\t2617929\n",
      "Unique Peptides:\t654023\n",
      "Sum: 3389443858279.8213\n",
      "Sum-Squared: 3.084857990014664e+16\n",
      "N: 294586320000\n",
      "mean: 11.505774804070404\n",
      "std: 323.39746666505516\n"
     ]
    }
   ],
   "source": [
    "# transformer = transforms.Normalize(mean=[3.725], std=[51.479])\n",
    "preprocess_mgfs(in_dir, in_tensor_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 325447965576.6338 / 25019920000\n",
    "print(\"mean: {}\".format(mean))\n",
    "std = math.sqrt((2885406787428155.0 / 25019920000) - mean**2)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledSpectra(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dir_path, filt, test=False):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.aas         = ['_PAD'] + list(config.AAMass.keys())\n",
    "        self.aa2idx      = {a:i for i, a in enumerate(self.aas)}\n",
    "        self.idx2aa      = {i:a for i, a in enumerate(self.aas)}\n",
    "        \n",
    "        self.spec_path   = join(dir_path, 'spectra')\n",
    "        self.pep_path    = join(dir_path, 'peptides')\n",
    "        self.charge      = filt['charge'] if 'charge' in filt else config.get_config(section='input', key='charge')\n",
    "        self.num_species = config.get_config(section='input', key='num_species')\n",
    "        # self.vocab_size  = len(self.aa2idx) + self.charge + self.num_species + 1\n",
    "        self.vocab_size  = round(max(config.AAMass.values())) + 1\n",
    "        self.seq_len     = config.get_config(section='ml', key='pep_seq_len')\n",
    "        self.modified    = filt['modified'] if 'modified' in filt else False\n",
    "        self.test_size   = config.get_config(section='ml', key='test_size')\n",
    "        self.test        = test\n",
    "        \n",
    "        self.file_names  = []\n",
    "        for file in listdir(self.spec_path):\n",
    "            if self.apply_filter(file):\n",
    "                self.file_names.append(file)\n",
    "        \n",
    "        print('dataset size: {}'.format(len(self.file_names)))        \n",
    "        \n",
    "        self.train_files, self.test_files = train_test_split(\n",
    "            self.file_names, test_size = self.test_size, random_state = rand.randint(0, 1000), shuffle=True)\n",
    "        \n",
    "        if self.test:\n",
    "            print('test size: {}'.format(len(self.test_files)))\n",
    "        else:\n",
    "            print('train size: {}'.format(len(self.train_files)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        if self.test:\n",
    "            return len(self.test_files)\n",
    "        else:\n",
    "            return len(self.train_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        file_name = ''\n",
    "        # Select sample\n",
    "        if self.test:\n",
    "            file_name = self.test_files[index]\n",
    "        else:\n",
    "            file_name = self.train_files[index]\n",
    "            \n",
    "        spec_file_name = join(self.spec_path, file_name)\n",
    "        pep_file_name  = join(self.pep_path, file_name.replace('.pt', '.pep'))\n",
    "        \n",
    "        # Load data and get label\n",
    "        spec_torch = torch.load(spec_file_name)\n",
    "        \n",
    "        # Load peptide and convert to idx array\n",
    "        f = open(pep_file_name, \"r\")\n",
    "        pep = f.readlines()[0].strip()\n",
    "        f.close()\n",
    "        \n",
    "        pepl = np.zeros(len(pep))\n",
    "        file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pt\", file_name)\n",
    "        pepl[0] = int(file_parts[4]) + len(self.aas)  # coded value of charge\n",
    "        pepl[1] = int(file_parts[2]) + self.charge + 1 + len(self.aas) # coded value of specie id\n",
    "        \n",
    "        # for i in range(2, len(pep)):\n",
    "        #     pepl[i] = self.aa2idx[pep[i]]\n",
    "        for i, aa in enumerate(pep[2:]):\n",
    "            pepl[i + 2] = self.aa2idx[aa]\n",
    "            # pepl[i + 2] = round(config.AAMass[aa])\n",
    "        \n",
    "        pepl = self.pad_left(pepl, self.seq_len)\n",
    "        pep_torch = torch.tensor(pepl, dtype=torch.long)\n",
    "        \n",
    "        return [spec_torch, pep_torch]\n",
    "    \n",
    "    def apply_filter(self, file_name):\n",
    "        file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pt\", file_name)\n",
    "        charge = int(file_parts[4])\n",
    "        modified = bool(int(file_parts[5]))\n",
    "        \n",
    "        if ((self.charge == 0 or charge <= self.charge)\n",
    "            and (self.modified or self.modified == modified)):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def pad_left(self, arr, size):\n",
    "        out = np.zeros(size)\n",
    "        out[-len(arr):] = arr\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_mean = np.mean(dataset)\n",
    "# data_std = np.std(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = config.get_config(section='ml', key='batch_size')\n",
    "batch_size = 256\n",
    "charge = config.get_config(section='input', key='charge')\n",
    "use_mods = config.get_config(section='input', key='use_mods')\n",
    "filt = {'charge':charge, 'modified':use_mods}\n",
    "\n",
    "\n",
    "train_dataset = LabeledSpectra(in_tensor_dir, filt, test=False)\n",
    "test_dataset = LabeledSpectra(in_tensor_dir, filt, test=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_learn = True\n",
    "save_frequency = 2\n",
    "lr = 0.0001\n",
    "num_epochs = 200\n",
    "weight_decay = 0.0001\n",
    "margin = 0.2\n",
    "vocab_size = train_dataset.vocab_size\n",
    "#torch.manual_seed(0)\n",
    "#torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = torch.load('train_loader.pt')\n",
    "#test_loader = torch.load('test_loader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size=512, embedding_dim=512, hidden_lstm_dim=1024, lstm_layers=2, drop_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.spec_size = config.get_config(section='input', key='spec_size')\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_lstm_dim = hidden_lstm_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.searching = False\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_lstm_dim, self.lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1_1 = nn.Linear(self.spec_size, 1024)\n",
    "        self.linear1_2 = nn.Linear(1024, 512)\n",
    "        self.linear1_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.linear2_1 = nn.Linear(2048, 1024)\n",
    "        self.linear2_2 = nn.Linear(1024, 512)\n",
    "        self.linear2_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        #self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, data, hidden):\n",
    "        specs = data[0]\n",
    "        peps = data[1]\n",
    "        # print(peps.type())\n",
    "        # print('Input to the model size: {}'.format(specs.size()))\n",
    "        # print('Input to the model size: {}'.format(peps.size()))\n",
    "        # peps = peps.unsqueeze(-1).float()\n",
    "        # print(peps.type())\n",
    "        \n",
    "        embeds = self.embedding(peps)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # print(lstm_out.size())\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        # print(lstm_out.size())\n",
    "        #lstm_out = torch.mean(lstm_out, dim=1)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_lstm_dim * 2)\n",
    "        out = self.dropout2(lstm_out)\n",
    "        \n",
    "        out = self.linear2_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.linear2_2(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # out = out.view(batch_size, peps.size()[1], 512)\n",
    "        # out = out[:,-1,:]\n",
    "        out_pep = F.normalize(out)\n",
    "        \n",
    "        out = self.linear1_1(specs.view(-1, self.spec_size))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear1_2(out)\n",
    "        out = F.relu(out)\n",
    "        out_spec = F.normalize(out)\n",
    "        \n",
    "        res = out_spec, out_pep, hidden\n",
    "        return res\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm_layers * 2, batch_size, self.hidden_lstm_dim).zero_().to(device),\n",
    "                      weight.new(self.lstm_layers * 2, batch_size, self.hidden_lstm_dim).zero_().to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hinge = torch.nn.HingeEmbeddingLoss()\n",
    "triplet_loss = nn.TripletMarginLoss(margin=margin, p=2, reduction='sum')\n",
    "l2_squared = nn.MSELoss(reduction='none')\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "zero_tensor = torch.tensor(0.).to(device)\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    accurate_labels = 0\n",
    "    all_labels = 0\n",
    "    for (batch_idx, data) in enumerate(train_loader):\n",
    "        h = tuple([e.data for e in h])\n",
    "        data[0], data[1] = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        Q, P, h = model(data, h)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \"\"\"Mine the hardest triplets. Get rid of N.\"\"\" \n",
    "        QxQ = process.pairwise_distances(Q)    # calculate distance matrix for spectra\n",
    "        PxP = process.pairwise_distances(P)    # calculate distance matrix for peptides\n",
    "        QxP_ = process.pairwise_distances(Q, P) # calculate distance matrix for spectra-peptides\n",
    "        \n",
    "        # Set the diagonal of all distance matrices to inf so we don't get self as the closest negative.\n",
    "        QxQ.fill_diagonal_(float(\"inf\"))\n",
    "        PxP.fill_diagonal_(float(\"inf\"))\n",
    "        QxP = QxP_.clone()\n",
    "        QxP.fill_diagonal_(float(\"inf\"))\n",
    "        \n",
    "        #print(QP.argmin(1)[:100])\n",
    "        \n",
    "#         pos = torch.sum(l2_squared(Q, P), dim=1) + margin\n",
    "        \n",
    "#         QxQ_min = QxQ.gather(1, torch.randint(len(Q), (len(Q),), device=device).view(-1,1))             # farthest spectrum for each spectrum\n",
    "#         PxP_min = PxP.gather(1, torch.randint(len(Q), (len(Q),), device=device).view(-1,1))             # farthest peptide for each peptide\n",
    "#         QxP_min = QxP.gather(1, torch.randint(len(Q), (len(Q),), device=device).view(-1,1))             # farthest peptide for each spectrum\n",
    "#         PxQ_min = QxP.gather(0, torch.randint(len(Q), (len(Q),), device=device).view(1,-1))             # farthest spectrum for each peptide\n",
    "        \n",
    "        QxQ_min = QxQ.min(1).values              # nearest spectrum for each spectrum\n",
    "        PxP_min = PxP.min(1).values              # nearest peptide for each peptide\n",
    "        QxP_min = QxP.min(1).values              # nearest peptide for each spectrum\n",
    "        PxQ_min = QxP.min(0).values              # nearest spectrum for each peptide\n",
    "        \n",
    "        #neg = QxQ_min + PxP_min + QxP_min + PxQ_min\n",
    "        \n",
    "#         divider = torch.tensor(float(len(pos)))\n",
    "#         loss = torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#         loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#         loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#         loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "        \n",
    "        #divider = torch.sum(pos - neg > 0)\n",
    "        #loss = torch.sum(torch.max(pos - neg, zero_tensor)) / divider\n",
    "        \n",
    "        QxQ_min = Q[QxQ.min(1).indices]              # nearest spectrum for each spectrum\n",
    "        PxP_min = P[PxP.min(1).indices]              # nearest peptide for each peptide\n",
    "        QxP_min = P[QxP.min(1).indices]              # nearest peptide for each spectrum\n",
    "        PxQ_min = Q[QxP.min(0).indices]              # nearest spectrum for each peptide\n",
    "        loss = triplet_loss(Q, P, QxQ_min)           # spectrum-spectrum negatives\n",
    "        loss += triplet_loss(Q, P, QxP_min)          # spectrum-peptide negatives\n",
    "        loss += triplet_loss(P, Q, PxP_min)          # peptide-peptide negatives\n",
    "        loss += triplet_loss(P, Q, PxQ_min)          # peptide-spectrum negatives\n",
    "        \n",
    "        loss = loss / 4\n",
    "                \n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        seq = torch.arange(0, len(Q), step=1, device=device, requires_grad=False)\n",
    "        accurate_labels = accurate_labels + torch.sum(QxP_.argmin(1) == seq) # use QP_ since it doesn't have diag set to zero\n",
    "        \n",
    "        all_labels = all_labels + len(Q)  \n",
    "    \n",
    "    accuracy = 100. * float(accurate_labels) / all_labels\n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(loss)\n",
    "    print('Epoch: ' + str(epoch))\n",
    "    print('Train accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        for (batch_idx, data) in enumerate(test_loader):\n",
    "            h = tuple([e.data for e in h])\n",
    "            data[0], data[1] = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Q, P, h = model(data, h)\n",
    "            \n",
    "            \"\"\"Mine the hardest triplets. Get rid of N.\"\"\" \n",
    "            QxQ = process.pairwise_distances(Q)    # calculate distance matrix for spectra\n",
    "            PxP = process.pairwise_distances(P)    # calculate distance matrix for peptides\n",
    "            QxP_ = process.pairwise_distances(Q, P) # calculate distance matrix for spectra-peptides\n",
    "\n",
    "            # Set the diagonal of all distance matrices to inf so we don't get self as the closest negative.\n",
    "            QxQ.fill_diagonal_(float(\"inf\"))\n",
    "            PxP.fill_diagonal_(float(\"inf\"))\n",
    "            QxP = QxP_.clone()    # clone to measure accuracy. can be done in a better way.\n",
    "            QxP.fill_diagonal_(float(\"inf\"))\n",
    "\n",
    "            #print(QP.argmin(1)[:100])\n",
    "\n",
    "            pos = 4 * (torch.sum(l2_squared(Q, P), dim=1) + margin)\n",
    "\n",
    "            QxQ_min = QxQ.min(1).values              # farthest spectrum for each spectrum\n",
    "            PxP_min = PxP.min(1).values              # farthest peptide for each peptide\n",
    "            QxP_min = QxP.min(1).values              # farthest peptide for each spectrum\n",
    "            PxQ_min = QxP.min(0).values              # farthest spectrum for each peptide\n",
    "\n",
    "            #neg = QxQ_min + PxP_min + QxP_min + PxQ_min\n",
    "        \n",
    "#             divider = torch.tensor(float(len(pos)))\n",
    "#             loss = torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "\n",
    "            QxQ_min = Q[QxQ.min(1).indices]              # nearest spectrum for each spectrum\n",
    "            PxP_min = P[PxP.min(1).indices]              # nearest peptide for each peptide\n",
    "            QxP_min = P[QxP.min(1).indices]              # nearest peptide for each spectrum\n",
    "            PxQ_min = Q[QxP.min(0).indices]              # nearest spectrum for each peptide\n",
    "            loss = triplet_loss(Q, P, QxQ_min)     # spectrum-spectrum negatives\n",
    "            loss += triplet_loss(Q, P, QxP_min)    # spectrum-peptide negatives\n",
    "            loss += triplet_loss(P, Q, PxP_min)    # peptide-peptide negatives\n",
    "            loss += triplet_loss(P, Q, PxQ_min)    # peptide-spectrum negatives\n",
    "            \n",
    "            loss = loss / 4\n",
    "\n",
    "            #divider = torch.tensor(float(len(pos)))\n",
    "            #divider = torch.sum(pos - neg > 0)\n",
    "            #loss = torch.sum(torch.max(pos - neg, zero_tensor)) / divider\n",
    "            \n",
    "#             loss =  torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "            \n",
    "            seq = torch.arange(0, len(Q), step=1, device=device, requires_grad=False)\n",
    "            accurate_labels = accurate_labels + torch.sum(QxP_.argmin(1) == seq) # use QP_ since it doesn't have diag set to zero\n",
    "            \n",
    "            all_labels = all_labels + len(Q)\n",
    "                \n",
    "        accuracy = 100. * float(accurate_labels) / all_labels\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_loss.append(loss)\n",
    "        print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "        output = model(data)\n",
    "        return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with redirect_output(\"deepSNAP_redirect.txt\"):\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "#drop_prob=0.5\n",
    "print(vocab_size)\n",
    "model = Net(vocab_size, output_size=512, embedding_dim=256, hidden_lstm_dim=1024, lstm_layers=1).to(device)\n",
    "# model.linear1_1.weight.requires_grad = False\n",
    "# model.linear1_1.bias.requires_grad = False\n",
    "# model.linear1_2.weight.requires_grad = False\n",
    "# model.linear1_2.bias.requires_grad = False\n",
    "\n",
    "if do_learn: # training mode\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, device, train_loader, epoch, optimizer)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/lstm_97.7%_v3.0.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove modifications  \n",
    "use one charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([[1., 1.], [1., 2.], [3., 4.]])\n",
    "S = torch.tensor([[2., 3.], [3., 2.], [4., 3.], [5, 3]])\n",
    "\n",
    "mul = torch.mm(T, S.t())\n",
    "print('mul: ')\n",
    "print(mul)\n",
    "adder = torch.tensor([1., 2., 3., 4.])\n",
    "added = adder + mul\n",
    "print('added: ')\n",
    "print(added)\n",
    "\n",
    "norm = T.pow(2).sum(1)\n",
    "print(norm)\n",
    "exp_norm = norm.expand(4, -1).t()\n",
    "print(exp_norm)\n",
    "# pdist = nn.PairwiseDistance(p=2)\n",
    "# output = pdist(T, S)\n",
    "# output\n",
    "# print(T)\n",
    "# print(T.t())\n",
    "# test = torch.tensor([1, 2, 3])\n",
    "# expanded_test = test.expand(3, -1)\n",
    "# print(expanded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(A, B):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = torch.mm(A, B.t())\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    \n",
    "    A_L2_norm = A.pow(2).sum(1)\n",
    "    B_L2_norm = B.pow(2).sum(1)\n",
    "    \n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = A_L2_norm[:, None] - (2.0 * dot_product) + B_L2_norm\n",
    "    \n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = torch.max(distances, torch.tensor(0.0))\n",
    "\n",
    "#     if not squared:\n",
    "#         # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "#         # we need to add a small epsilon where distances == 0.0\n",
    "#         mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "#         distances = distances + mask * 1e-16\n",
    "\n",
    "#         distances = tf.sqrt(distances)\n",
    "\n",
    "#         # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "#         distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = pairwise_distance(T, T)\n",
    "print(T)\n",
    "print(S)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([[1., 1.], [1., 2.], [3., 4.]])  \n",
    "S = torch.tensor([[2., 3.], [3., 2.], [4., 3.], [5, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(x, y=None):\n",
    "    '''\n",
    "    Input: x is a Nxd matrix\n",
    "           y is an optional Mxd matirx\n",
    "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "            if y is not given then use 'y=x'.\n",
    "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "    '''\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    if y is not None:\n",
    "        y_t = torch.transpose(y, 0, 1)\n",
    "        y_norm = (y**2).sum(1).view(1, -1)\n",
    "    else:\n",
    "        y_t = torch.transpose(x, 0, 1)\n",
    "        y_norm = x_norm.view(1, -1)\n",
    "    \n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    # Ensure diagonal is zero if x=y\n",
    "    if y is None:\n",
    "        dist = dist - torch.diag(dist.diag())\n",
    "    dist[dist != dist] = 0 # set all nan values to zero\n",
    "    return torch.clamp(dist, 0.0, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 1.4335, -1.0990, -0.8586],\n",
    "        [ 2.1553,  2.7028, -0.8020],\n",
    "        [ 1.0524,  0.1599, -0.0374]])\n",
    "b = torch.tensor([[ 3., 7., 2.],\n",
    "                [ 6.,  8., 2.],\n",
    "                [ 9.,  5., 7.]])\n",
    "dists = process.pairwise_distances(x=a, y=None)\n",
    "print(dists)\n",
    "print(b.max(1).values)\n",
    "print(b[b.max(1).indices])\n",
    "#print(b.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 3., 1., 4.],\n",
    "                [ 0.,  2., 3.],\n",
    "                [ 5.,  3., 6.]])\n",
    "b = torch.tensor([[ 1., 2., 1.],\n",
    "                [ 2.,  2., 1.],\n",
    "                [ 1.,  1., 1.]])\n",
    "dist = (a - b) ** 2\n",
    "print(l2_squared(a, b))\n",
    "print(torch.sum(l2_squared(a, b), 1))\n",
    "print(a.min(1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = torch.tensor([1., -1., 3.])\n",
    "max0 = torch.max(a0, torch.tensor(0.))\n",
    "print(max0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([1, 2, 3, 4])\n",
    "b = np.asarray([0, 1, 0])\n",
    "a.append(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_print(i):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_path = join(in_tensor_dir, 'peptides')\n",
    "charge1 = 0\n",
    "charge2 = 0\n",
    "charge3 = 0\n",
    "for file in listdir(pep_path):\n",
    "    file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pep\", file)\n",
    "    charge = int(file_parts[4])\n",
    "    if charge == 1:\n",
    "        charge1 += 1\n",
    "    elif charge == 2:\n",
    "        charge2 += 1\n",
    "    elif charge == 3:\n",
    "        charge3 += 1\n",
    "print('charge 1 count: {}'.format(charge1))\n",
    "print('charge 2 count: {}'.format(charge2))\n",
    "print('charge 3 count: {}'.format(charge3))\n",
    "\n",
    "unmod = 0\n",
    "mod = 0\n",
    "for file in listdir(pep_path):\n",
    "    file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pep\", file)\n",
    "    m = int(file_parts[5])\n",
    "    if m == 0:\n",
    "        unmod += 1\n",
    "    elif m == 1:\n",
    "        mod += 1\n",
    "print('Unmodified count: {}'.format(unmod))\n",
    "print('Modified count: {}'.format(mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Work Below This:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = torch.FloatTensor([1., 2., 3., 4., 5.]).unsqueeze(dim=0)\n",
    "s2 = torch.FloatTensor([6., 7., 8., 9., 10.]).unsqueeze(dim=0)\n",
    "print(s1)\n",
    "torch.cat((s1, s2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = torch.LongTensor([[0]*5, [1, 3, 5, 7, 9]])\n",
    "val = torch.FloatTensor([1., 1., 1., 1., 1.])\n",
    "spec = torch.sparse_coo_tensor(ind, val, torch.Size([1, 10]))\n",
    "print(spec.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 5\n",
    "width = 15\n",
    "count = np.array([5, 3, 4, 1, 2])\n",
    "PQ_mask = torch.zeros(length, width)\n",
    "\n",
    "rows = []\n",
    "[rows.extend([i]*x) for i,x in enumerate(count)]\n",
    "cols = range(width)\n",
    "PQ_mask[rows, cols] = 1.\n",
    "print(c_sums)\n",
    "print(PQ_mask)\n",
    "QQ_mask = torch.zeros(width, width)\n",
    "QQ_mask[cols, :] = PQ_mask[rows, :]\n",
    "print(QQ_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "[rows.extend([i]*x) for i,x in enumerate(count)]\n",
    "cols = range(0, 15)\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "101.071365 2082.4314\n"
     ]
    }
   ],
   "source": [
    "test = \"101.071365 2082.4314\"\n",
    "sp = test.split(' \\t=')\n",
    "print(len(sp))\n",
    "print(sp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
