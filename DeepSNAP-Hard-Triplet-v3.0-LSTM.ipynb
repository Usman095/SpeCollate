{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import queue\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "import csv\n",
    "from heapq import merge\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "from configparser import ConfigParser\n",
    "import ast\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "rand.seed(37)\n",
    "\n",
    "#from src.snapconfig import config\n",
    "from src.snapprocess import simulatespectra as sim\n",
    "from src.snapprocess import process\n",
    "from src.snaputils import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary config func. Original one in the project.\n",
    "class config:\n",
    "    \"\"\"Define constants\"\"\"\n",
    "    AAMass = {'A': 71.037114, 'C': 103.009185, 'D': 115.026943, 'E': 129.042593, 'F': 147.068414, 'G': 57.021464,\n",
    "              'H': 137.058912, 'I': 113.084064, 'K': 128.094963, 'L': 113.084064, 'M': 131.040485, 'N': 114.042927,\n",
    "              'P': 97.052764, 'Q': 128.058578, 'R': 156.101111, 'S': 87.032028, 'T': 101.047679, 'V': 99.068414,\n",
    "              'W': 186.079313, 'Y': 163.0633}\n",
    "\n",
    "    H2O = 18.015\n",
    "    NH3 = 17.031\n",
    "    PROTON = 1.00727647\n",
    "    DEFAULT_PARAM_PATH = os.path.join(os.getcwd(), 'config.ini')\n",
    "    PARAM_PATH = None\n",
    "    l_config = None\n",
    "\n",
    "\n",
    "    def get_config(section='input', key=None):\n",
    "        \"\"\"Read the configuration parameters and return a dictionary.\"\"\"\n",
    "\n",
    "        # If file path is given use it otherwise use default.\n",
    "        file_path = config.PARAM_PATH if config.PARAM_PATH else config.DEFAULT_PARAM_PATH\n",
    "\n",
    "        # Read config and convert each value to appropriate type.\n",
    "        # Only for the first time.\n",
    "        if not config.l_config:\n",
    "            config.l_config = dict()\n",
    "            config_ = ConfigParser()\n",
    "            assert isinstance(file_path, str)\n",
    "            config_.read(file_path)\n",
    "            for section_ in config_.sections():\n",
    "                config.l_config[section_] = dict()\n",
    "                for key_ in config_[section_]:\n",
    "                    try:\n",
    "                        config.l_config[section_][key_] = ast.literal_eval(config_[section_][key_])\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        config.l_config[section_][key_] = config_[section_][key_]\n",
    "\n",
    "        if section and section in config.l_config:\n",
    "            if key and key in config.l_config[section]:\n",
    "                return config.l_config[section][key]\n",
    "            return config.l_config[section]\n",
    "        return config.l_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_msps(msp_dir, out_dir):\n",
    "    in_path = Path(msp_dir)\n",
    "    assert in_path.exists() and in_path.is_dir()\n",
    "    \n",
    "    msp_files = [join(msp_dir, f) for f in listdir(msp_dir) if\n",
    "                 isfile(join(msp_dir, f)) and f.split('.')[-1] == 'msp']\n",
    "    assert len(msp_files) > 0\n",
    "    \n",
    "    out_path = Path(out_dir)\n",
    "    if out_path.exists() and out_path.is_dir():\n",
    "        shutil.rmtree(out_path)\n",
    "    out_path.mkdir()\n",
    "    Path(join(out_path, 'spectra')).mkdir()\n",
    "    Path(join(out_path, 'peptides')).mkdir()\n",
    "        \n",
    "    print('reading {} files'.format(len(msp_files)))\n",
    "    \n",
    "    count = 0\n",
    "    max_peaks = max_moz = 0\n",
    "    for species_id, msp_file in enumerate(msp_files):\n",
    "        print('Reading: {}'.format(msp_file))\n",
    "        \n",
    "        f = open(msp_file, \"r\")\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        pep_list = []\n",
    "        dataset = []\n",
    "        label = []\n",
    "\n",
    "        # FIXME: config should use only one get_config call.\n",
    "        spec_size = config.get_config(section='input', key='spec_size')\n",
    "        charge = config.get_config(section='input', key='charge')\n",
    "        use_mods = config.get_config(section='input', key='use_mods')\n",
    "        num_species = config.get_config(section='input', key='num_species')\n",
    "        seq_len = config.get_config(section='ml', key='pep_seq_len')\n",
    "\n",
    "        print('len of file: ' + str(len(lines)))\n",
    "        limit = 200000\n",
    "        pep = []\n",
    "        spec = []\n",
    "        pep_set = set()\n",
    "        is_name = is_mw = is_num_peaks = False\n",
    "        prev = 0\n",
    "        i = 0\n",
    "        while i < len(lines) and limit > 0:\n",
    "            line = lines[i]\n",
    "            i += 1\n",
    "            if line.startswith('Name:'):\n",
    "                name_groups = re.search(r\"Name:\\s(?P<pep>[a-zA-Z]+)/(?P<charge>\\d+)\"\n",
    "                                        r\"(?:_(?P<num_mods>\\d+)(?P<mods>.*))?\", line)\n",
    "                if not name_groups:\n",
    "                    continue\n",
    "                    \n",
    "                pep = name_groups['pep']\n",
    "                if len(pep) + 1 > seq_len:\n",
    "                    continue\n",
    "                    \n",
    "                l_charge = int(name_groups['charge'])\n",
    "                num_mods = int(name_groups['num_mods'])\n",
    "\n",
    "                is_name = True\n",
    "\n",
    "            if is_name and line.startswith('MW:'):\n",
    "                mass = float(re.findall(r\"MW:\\s([-+]?[0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                if round(mass) < spec_size:\n",
    "                    is_mw = True\n",
    "                    # limit = limit - 1\n",
    "                else:\n",
    "                    is_name = is_mw = is_num_peaks = False\n",
    "                    continue\n",
    "\n",
    "            if is_name and is_mw and line.startswith('Num peaks:'):\n",
    "                num_peaks = int(re.findall(r\"Num peaks:\\s([0-9]*\\.?[0-9]*)\", line)[0])\n",
    "                if num_peaks > max_peaks:\n",
    "                    max_peaks = num_peaks\n",
    "\n",
    "                spec = np.zeros(spec_size)\n",
    "                while lines[i] != '\\n':\n",
    "                    mz_line = lines[i]\n",
    "                    i += 1\n",
    "                    mz_splits = mz_line.split('\\t')\n",
    "                    moz, intensity = float(mz_splits[0]), float(mz_splits[1])\n",
    "                    if moz > max_moz:\n",
    "                        max_moz = moz\n",
    "                    spec[round(moz)] += round(intensity)\n",
    "\n",
    "                # for k in range(1, charge + 1):\n",
    "                #     spec[-k] = 0\n",
    "                # spec[-l_charge] = 1000.0\n",
    "                spec = np.clip(spec, None, 1000.0)\n",
    "                # spec = preprocessing.scale(spec)\n",
    "\n",
    "                is_num_peaks = True\n",
    "\n",
    "            if is_name and is_mw and is_num_peaks:\n",
    "                is_name = is_mw = is_num_peaks = False\n",
    "                \n",
    "                #pep = '{}{}{}'.format(charge, species_id, pep)\n",
    "\n",
    "                \"\"\"output the data to \"\"\"\n",
    "                spec_tensor = torch.tensor((np.asarray(spec) - 3.725) / 51.479, dtype=torch.float)\n",
    "                \n",
    "                torch.save(spec_tensor, \n",
    "                           join(out_dir, 'spectra', '{}-{}-{}-{}-{}.pt'\n",
    "                                .format(count, species_id, mass, l_charge, int(num_mods > 0))))\n",
    "                \n",
    "                pep_file_name = '{}-{}-{}-{}-{}.pep'.format(count, species_id, mass, l_charge, int(num_mods > 0))\n",
    "                    \n",
    "                with open(join(out_path, 'peptides', pep_file_name), 'w+') as f:\n",
    "                    f.write(pep)\n",
    "\n",
    "                count = count + 1\n",
    "                pep = 0\n",
    "                spec = []\n",
    "                new = int((i / len(lines)) * 100)\n",
    "                if new > prev:\n",
    "                    clear_output(wait=True)\n",
    "                    print(str(new) + '%')\n",
    "                    prev = new\n",
    "\n",
    "        print('max peaks: ' + str(max_peaks))\n",
    "        print('count: ' + str(count))\n",
    "        print('max moz: ' + str(max_moz))\n",
    "#         return pep_list, dataset, label\n",
    "#         tmp_pep_list, tmp_dataset, tmp_labels = read_msp(msp_file, species_id, decoy)\n",
    "#         pep_list.extend(tmp_dataset)\n",
    "#         dataset.extend(tmp_dataset)\n",
    "#         label.extend(tmp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_dir = \"/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/msp-labeled/\"\n",
    "in_tensor_dir = \"/disk/raptor/lclhome/mtari008/Proteogenomics/DeepSNAP/data/train_lstm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = transforms.Normalize(mean=[3.725], std=[51.479])\n",
    "# preprocess_msps(msp_dir, in_tensor_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledSpectra(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, dir_path, filt, test=False):\n",
    "        'Initialization'\n",
    "        \n",
    "        self.aas         = ['_PAD'] + list(config.AAMass.keys())\n",
    "        self.aa2idx      = {a:i for i, a in enumerate(self.aas)}\n",
    "        self.idx2aa      = {i:a for i, a in enumerate(self.aas)}\n",
    "        \n",
    "        self.spec_path   = join(dir_path, 'spectra')\n",
    "        self.pep_path    = join(dir_path, 'peptides')\n",
    "        self.charge      = filt['charge'] if 'charge' in filt else config.get_config(section='input', key='charge')\n",
    "        self.num_species = config.get_config(section='input', key='num_species')\n",
    "        # self.vocab_size  = len(self.aa2idx) + self.charge + self.num_species + 1\n",
    "        self.vocab_size  = round(max(config.AAMass.values())) + 1\n",
    "        self.seq_len     = config.get_config(section='ml', key='pep_seq_len')\n",
    "        self.modified    = filt['modified'] if 'modified' in filt else False\n",
    "        self.test_size   = config.get_config(section='ml', key='test_size')\n",
    "        self.test        = test\n",
    "        \n",
    "        self.file_names  = []\n",
    "        for file in listdir(self.spec_path):\n",
    "            if self.apply_filter(file):\n",
    "                self.file_names.append(file)\n",
    "        \n",
    "        print('dataset size: {}'.format(len(self.file_names)))        \n",
    "        \n",
    "        self.train_files, self.test_files = train_test_split(\n",
    "            self.file_names, test_size = self.test_size, random_state = rand.randint(0, 1000), shuffle = True)\n",
    "        \n",
    "        if self.test:\n",
    "            print('test size: {}'.format(len(self.test_files)))\n",
    "        else:\n",
    "            print('train size: {}'.format(len(self.train_files)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        if self.test:\n",
    "            return len(self.test_files)\n",
    "        else:\n",
    "            return len(self.train_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        file_name = ''\n",
    "        # Select sample\n",
    "        if self.test:\n",
    "            file_name = self.test_files[index]\n",
    "        else:\n",
    "            file_name = self.train_files[index]\n",
    "            \n",
    "        spec_file_name = join(self.spec_path, file_name)\n",
    "        pep_file_name  = join(self.pep_path, file_name.replace('.pt', '.pep'))\n",
    "        \n",
    "        # Load data and get label\n",
    "        spec_torch = torch.load(spec_file_name)\n",
    "        \n",
    "        # Load peptide and convert to idx array\n",
    "        f = open(pep_file_name, \"r\")\n",
    "        pep = f.readlines()[0].strip()\n",
    "        f.close()\n",
    "        \n",
    "        pepl = np.zeros(len(pep))\n",
    "        file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pt\", file_name)\n",
    "        pepl[0] = int(file_parts[4]) + len(self.aas)  # coded value of charge\n",
    "        pepl[1] = int(file_parts[2]) + self.charge + 1 + len(self.aas) # coded value of specie id\n",
    "        \n",
    "        # for i in range(2, len(pep)):\n",
    "        #     pepl[i] = self.aa2idx[pep[i]]\n",
    "        for i, aa in enumerate(pep[2:]):\n",
    "            pepl[i + 2] = self.aa2idx[aa]\n",
    "            # pepl[i + 2] = round(config.AAMass[aa])\n",
    "        \n",
    "        pepl = self.pad_left(pepl, self.seq_len)\n",
    "        pep_torch = torch.tensor(pepl, dtype=torch.long)\n",
    "        \n",
    "        return [spec_torch, pep_torch]\n",
    "    \n",
    "    def apply_filter(self, file_name):\n",
    "        file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pt\", file_name)\n",
    "        charge = int(file_parts[4])\n",
    "        modified = bool(int(file_parts[5]))\n",
    "        \n",
    "        if ((self.charge == 0 or charge <= self.charge)\n",
    "            and (self.modified or self.modified == modified)):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def pad_left(self, arr, size):\n",
    "        out = np.zeros(size)\n",
    "        out[-len(arr):] = arr\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_mean = np.mean(dataset)\n",
    "# data_std = np.std(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 587162\n",
      "train size: 469729\n",
      "dataset size: 587162\n",
      "test size: 117433\n"
     ]
    }
   ],
   "source": [
    "# batch_size = config.get_config(section='ml', key='batch_size')\n",
    "batch_size = 256\n",
    "charge = config.get_config(section='input', key='charge')\n",
    "use_mods = config.get_config(section='input', key='use_mods')\n",
    "filt = {'charge':charge, 'modified':use_mods}\n",
    "\n",
    "\n",
    "train_dataset = LabeledSpectra(in_tensor_dir, filt, test=False)\n",
    "test_dataset = LabeledSpectra(in_tensor_dir, filt, test=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_learn = True\n",
    "save_frequency = 2\n",
    "lr = 0.0001\n",
    "num_epochs = 200\n",
    "weight_decay = 0.0001\n",
    "margin = 0.2\n",
    "vocab_size = train_dataset.vocab_size\n",
    "#torch.manual_seed(0)\n",
    "#torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = torch.load('train_loader.pt')\n",
    "#test_loader = torch.load('test_loader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size=512, embedding_dim=512, hidden_lstm_dim=1024, lstm_layers=2, drop_prob=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.spec_size = config.get_config(section='input', key='spec_size')\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_lstm_dim = hidden_lstm_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.searching = False\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_lstm_dim, self.lstm_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1_1 = nn.Linear(self.spec_size, 1024)\n",
    "        self.linear1_2 = nn.Linear(1024, 512)\n",
    "        self.linear1_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.linear2_1 = nn.Linear(2048, 1024)\n",
    "        self.linear2_2 = nn.Linear(1024, 512)\n",
    "        self.linear2_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        #self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, data, hidden):\n",
    "        specs = data[0]\n",
    "        peps = data[1]\n",
    "        # print(peps.type())\n",
    "        # print('Input to the model size: {}'.format(specs.size()))\n",
    "        # print('Input to the model size: {}'.format(peps.size()))\n",
    "        # peps = peps.unsqueeze(-1).float()\n",
    "        # print(peps.type())\n",
    "        \n",
    "        embeds = self.embedding(peps)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # print(lstm_out.size())\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        # print(lstm_out.size())\n",
    "        #lstm_out = torch.mean(lstm_out, dim=1)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_lstm_dim * 2)\n",
    "        out = self.dropout2(lstm_out)\n",
    "        \n",
    "        out = self.linear2_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.linear2_2(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # out = out.view(batch_size, peps.size()[1], 512)\n",
    "        # out = out[:,-1,:]\n",
    "        out_pep = F.normalize(out)\n",
    "        \n",
    "        out = self.linear1_1(specs.view(-1, self.spec_size))\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.linear1_2(out)\n",
    "        out = F.relu(out)\n",
    "        out_spec = F.normalize(out)\n",
    "        \n",
    "        res = out_spec, out_pep, hidden\n",
    "        return res\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.lstm_layers * 2, batch_size, self.hidden_lstm_dim).zero_().to(device),\n",
    "                      weight.new(self.lstm_layers * 2, batch_size, self.hidden_lstm_dim).zero_().to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hinge = torch.nn.HingeEmbeddingLoss()\n",
    "triplet_loss = nn.TripletMarginLoss(margin=margin, p=2, reduction='sum')\n",
    "l2_squared = nn.MSELoss(reduction='none')\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "zero_tensor = torch.tensor(0.).to(device)\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    accurate_labels = 0\n",
    "    all_labels = 0\n",
    "    for (batch_idx, data) in enumerate(train_loader):\n",
    "        h = tuple([e.data for e in h])\n",
    "        data[0], data[1] = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        Q, P, h = model(data, h)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \"\"\"Mine the hardest triplets. Get rid of N.\"\"\" \n",
    "        QxQ = process.pairwise_distances(Q)    # calculate distance matrix for spectra\n",
    "        PxP = process.pairwise_distances(P)    # calculate distance matrix for peptides\n",
    "        QxP_ = process.pairwise_distances(Q, P) # calculate distance matrix for spectra-peptides\n",
    "        \n",
    "        # Set the diagonal of all distance matrices to inf so we don't get self as the closest negative.\n",
    "        QxQ.fill_diagonal_(float(\"inf\"))\n",
    "        PxP.fill_diagonal_(float(\"inf\"))\n",
    "        QxP = QxP_.clone()\n",
    "        QxP.fill_diagonal_(float(\"inf\"))\n",
    "        \n",
    "        #print(QP.argmin(1)[:100])\n",
    "        \n",
    "#         pos = torch.sum(l2_squared(Q, P), dim=1) + margin\n",
    "        \n",
    "#         QxQ_min = QxQ.gather(1, torch.randint(len(Q), (len(Q),), device=device).view(-1,1))             # farthest spectrum for each spectrum\n",
    "#         PxP_min = PxP.gather(1, torch.randint(len(Q), (len(Q),), device=device).view(-1,1))             # farthest peptide for each peptide\n",
    "#         QxP_min = QxP.gather(1, torch.randint(len(Q), (len(Q),), device=device).view(-1,1))             # farthest peptide for each spectrum\n",
    "#         PxQ_min = QxP.gather(0, torch.randint(len(Q), (len(Q),), device=device).view(1,-1))             # farthest spectrum for each peptide\n",
    "        \n",
    "        QxQ_min = QxQ.min(1).values              # nearest spectrum for each spectrum\n",
    "        PxP_min = PxP.min(1).values              # nearest peptide for each peptide\n",
    "        QxP_min = QxP.min(1).values              # nearest peptide for each spectrum\n",
    "        PxQ_min = QxP.min(0).values              # nearest spectrum for each peptide\n",
    "        \n",
    "        #neg = QxQ_min + PxP_min + QxP_min + PxQ_min\n",
    "        \n",
    "#         divider = torch.tensor(float(len(pos)))\n",
    "#         loss = torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#         loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#         loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#         loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "        \n",
    "        #divider = torch.sum(pos - neg > 0)\n",
    "        #loss = torch.sum(torch.max(pos - neg, zero_tensor)) / divider\n",
    "        \n",
    "        QxQ_min = Q[QxQ.min(1).indices]              # nearest spectrum for each spectrum\n",
    "        PxP_min = P[PxP.min(1).indices]              # nearest peptide for each peptide\n",
    "        QxP_min = P[QxP.min(1).indices]              # nearest peptide for each spectrum\n",
    "        PxQ_min = Q[QxP.min(0).indices]              # nearest spectrum for each peptide\n",
    "        loss = triplet_loss(Q, P, QxQ_min)           # spectrum-spectrum negatives\n",
    "        loss += triplet_loss(Q, P, QxP_min)          # spectrum-peptide negatives\n",
    "        loss += triplet_loss(P, Q, PxP_min)          # peptide-peptide negatives\n",
    "        loss += triplet_loss(P, Q, PxQ_min)          # peptide-spectrum negatives\n",
    "        \n",
    "        loss = loss / 4\n",
    "                \n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        seq = torch.arange(0, len(Q), step=1, device=device, requires_grad=False)\n",
    "        accurate_labels = accurate_labels + torch.sum(QxP_.argmin(1) == seq) # use QP_ since it doesn't have diag set to zero\n",
    "        \n",
    "        all_labels = all_labels + len(Q)  \n",
    "    \n",
    "    accuracy = 100. * float(accurate_labels) / all_labels\n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(loss)\n",
    "    print('Epoch: ' + str(epoch))\n",
    "    print('Train accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        h = model.init_hidden(batch_size)\n",
    "        \n",
    "        for (batch_idx, data) in enumerate(test_loader):\n",
    "            h = tuple([e.data for e in h])\n",
    "            data[0], data[1] = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Q, P, h = model(data, h)\n",
    "            \n",
    "            \"\"\"Mine the hardest triplets. Get rid of N.\"\"\" \n",
    "            QxQ = process.pairwise_distances(Q)    # calculate distance matrix for spectra\n",
    "            PxP = process.pairwise_distances(P)    # calculate distance matrix for peptides\n",
    "            QxP_ = process.pairwise_distances(Q, P) # calculate distance matrix for spectra-peptides\n",
    "\n",
    "            # Set the diagonal of all distance matrices to inf so we don't get self as the closest negative.\n",
    "            QxQ.fill_diagonal_(float(\"inf\"))\n",
    "            PxP.fill_diagonal_(float(\"inf\"))\n",
    "            QxP = QxP_.clone()    # clone to measure accuracy. can be done in a better way.\n",
    "            QxP.fill_diagonal_(float(\"inf\"))\n",
    "\n",
    "            #print(QP.argmin(1)[:100])\n",
    "\n",
    "            pos = 4 * (torch.sum(l2_squared(Q, P), dim=1) + margin)\n",
    "\n",
    "            QxQ_min = QxQ.min(1).values              # farthest spectrum for each spectrum\n",
    "            PxP_min = PxP.min(1).values              # farthest peptide for each peptide\n",
    "            QxP_min = QxP.min(1).values              # farthest peptide for each spectrum\n",
    "            PxQ_min = QxP.min(0).values              # farthest spectrum for each peptide\n",
    "\n",
    "            #neg = QxQ_min + PxP_min + QxP_min + PxQ_min\n",
    "        \n",
    "#             divider = torch.tensor(float(len(pos)))\n",
    "#             loss = torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "\n",
    "            QxQ_min = Q[QxQ.min(1).indices]              # nearest spectrum for each spectrum\n",
    "            PxP_min = P[PxP.min(1).indices]              # nearest peptide for each peptide\n",
    "            QxP_min = P[QxP.min(1).indices]              # nearest peptide for each spectrum\n",
    "            PxQ_min = Q[QxP.min(0).indices]              # nearest spectrum for each peptide\n",
    "            loss = triplet_loss(Q, P, QxQ_min)     # spectrum-spectrum negatives\n",
    "            loss += triplet_loss(Q, P, QxP_min)    # spectrum-peptide negatives\n",
    "            loss += triplet_loss(P, Q, PxP_min)    # peptide-peptide negatives\n",
    "            loss += triplet_loss(P, Q, PxQ_min)    # peptide-spectrum negatives\n",
    "            \n",
    "            loss = loss / 4\n",
    "\n",
    "            #divider = torch.tensor(float(len(pos)))\n",
    "            #divider = torch.sum(pos - neg > 0)\n",
    "            #loss = torch.sum(torch.max(pos - neg, zero_tensor)) / divider\n",
    "            \n",
    "#             loss =  torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "            \n",
    "            seq = torch.arange(0, len(Q), step=1, device=device, requires_grad=False)\n",
    "            accurate_labels = accurate_labels + torch.sum(QxP_.argmin(1) == seq) # use QP_ since it doesn't have diag set to zero\n",
    "            \n",
    "            all_labels = all_labels + len(Q)\n",
    "                \n",
    "        accuracy = 100. * float(accurate_labels) / all_labels\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_loss.append(loss)\n",
    "        print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "        output = model(data)\n",
    "        return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "Epoch: 0\n",
      "Train accuracy: 12897/469504 (2.747%)\tLoss: 52.419701\n",
      "Test accuracy: 30348/117248 (25.884%)\tLoss: 51.912865\n",
      "Epoch: 1\n",
      "Train accuracy: 72499/469504 (15.442%)\tLoss: 51.678970\n",
      "Test accuracy: 51587/117248 (43.998%)\tLoss: 51.370903\n",
      "Epoch: 2\n",
      "Train accuracy: 204240/469504 (43.501%)\tLoss: 49.222218\n",
      "Test accuracy: 77877/117248 (66.421%)\tLoss: 46.636387\n",
      "Epoch: 3\n",
      "Train accuracy: 314567/469504 (67.000%)\tLoss: 34.650257\n",
      "Test accuracy: 94243/117248 (80.379%)\tLoss: 26.470903\n",
      "Epoch: 4\n",
      "Train accuracy: 365597/469504 (77.869%)\tLoss: 29.054005\n",
      "Test accuracy: 100180/117248 (85.443%)\tLoss: 20.521399\n",
      "Epoch: 5\n",
      "Train accuracy: 387634/469504 (82.562%)\tLoss: 25.808140\n",
      "Test accuracy: 103421/117248 (88.207%)\tLoss: 17.253811\n",
      "Epoch: 6\n",
      "Train accuracy: 401285/469504 (85.470%)\tLoss: 21.894783\n",
      "Test accuracy: 105527/117248 (90.003%)\tLoss: 14.608198\n",
      "Epoch: 7\n",
      "Train accuracy: 411182/469504 (87.578%)\tLoss: 19.986324\n",
      "Test accuracy: 107205/117248 (91.434%)\tLoss: 13.615899\n",
      "Epoch: 8\n",
      "Train accuracy: 418643/469504 (89.167%)\tLoss: 16.206528\n",
      "Test accuracy: 108366/117248 (92.425%)\tLoss: 12.282682\n",
      "Epoch: 9\n",
      "Train accuracy: 424732/469504 (90.464%)\tLoss: 20.030472\n",
      "Test accuracy: 109326/117248 (93.243%)\tLoss: 11.506824\n",
      "Epoch: 10\n",
      "Train accuracy: 429442/469504 (91.467%)\tLoss: 15.667059\n",
      "Test accuracy: 110138/117248 (93.936%)\tLoss: 10.173656\n",
      "Epoch: 11\n",
      "Train accuracy: 433699/469504 (92.374%)\tLoss: 13.963419\n",
      "Test accuracy: 110635/117248 (94.360%)\tLoss: 10.137310\n",
      "Epoch: 12\n",
      "Train accuracy: 436967/469504 (93.070%)\tLoss: 11.218962\n",
      "Test accuracy: 111201/117248 (94.843%)\tLoss: 9.472480\n",
      "Epoch: 13\n",
      "Train accuracy: 439974/469504 (93.710%)\tLoss: 10.589298\n",
      "Test accuracy: 111583/117248 (95.168%)\tLoss: 8.613270\n",
      "Epoch: 14\n",
      "Train accuracy: 442395/469504 (94.226%)\tLoss: 10.318831\n",
      "Test accuracy: 111989/117248 (95.515%)\tLoss: 8.742523\n",
      "Epoch: 15\n",
      "Train accuracy: 444600/469504 (94.696%)\tLoss: 11.287182\n",
      "Test accuracy: 112212/117248 (95.705%)\tLoss: 7.826697\n",
      "Epoch: 16\n",
      "Train accuracy: 446451/469504 (95.090%)\tLoss: 12.100353\n",
      "Test accuracy: 112513/117248 (95.962%)\tLoss: 7.171565\n",
      "Epoch: 17\n",
      "Train accuracy: 448219/469504 (95.466%)\tLoss: 10.438650\n",
      "Test accuracy: 112741/117248 (96.156%)\tLoss: 7.418959\n",
      "Epoch: 18\n",
      "Train accuracy: 449536/469504 (95.747%)\tLoss: 9.169218\n",
      "Test accuracy: 112952/117248 (96.336%)\tLoss: 7.194267\n",
      "Epoch: 19\n",
      "Train accuracy: 451196/469504 (96.101%)\tLoss: 9.385660\n",
      "Test accuracy: 113119/117248 (96.478%)\tLoss: 6.922837\n",
      "Epoch: 20\n",
      "Train accuracy: 452112/469504 (96.296%)\tLoss: 10.363010\n",
      "Test accuracy: 113272/117248 (96.609%)\tLoss: 6.813765\n",
      "Epoch: 21\n",
      "Train accuracy: 453389/469504 (96.568%)\tLoss: 10.732210\n",
      "Test accuracy: 113436/117248 (96.749%)\tLoss: 6.683146\n",
      "Epoch: 22\n",
      "Train accuracy: 454093/469504 (96.718%)\tLoss: 8.161338\n",
      "Test accuracy: 113521/117248 (96.821%)\tLoss: 6.136650\n",
      "Epoch: 23\n",
      "Train accuracy: 455074/469504 (96.927%)\tLoss: 6.520853\n",
      "Test accuracy: 113644/117248 (96.926%)\tLoss: 6.181035\n",
      "Epoch: 24\n",
      "Train accuracy: 455980/469504 (97.120%)\tLoss: 8.624197\n",
      "Test accuracy: 113765/117248 (97.029%)\tLoss: 6.142305\n",
      "Epoch: 25\n",
      "Train accuracy: 456774/469504 (97.289%)\tLoss: 8.511055\n",
      "Test accuracy: 113855/117248 (97.106%)\tLoss: 5.735595\n",
      "Epoch: 26\n",
      "Train accuracy: 457382/469504 (97.418%)\tLoss: 6.788832\n",
      "Test accuracy: 113924/117248 (97.165%)\tLoss: 5.812980\n",
      "Epoch: 27\n",
      "Train accuracy: 458254/469504 (97.604%)\tLoss: 7.021553\n",
      "Test accuracy: 113948/117248 (97.185%)\tLoss: 5.272754\n",
      "Epoch: 28\n",
      "Train accuracy: 458770/469504 (97.714%)\tLoss: 6.769545\n",
      "Test accuracy: 113970/117248 (97.204%)\tLoss: 5.785162\n",
      "Epoch: 29\n",
      "Train accuracy: 459342/469504 (97.836%)\tLoss: 8.961357\n",
      "Test accuracy: 114066/117248 (97.286%)\tLoss: 5.470542\n",
      "Epoch: 30\n",
      "Train accuracy: 459723/469504 (97.917%)\tLoss: 4.623753\n",
      "Test accuracy: 114084/117248 (97.301%)\tLoss: 5.135633\n",
      "Epoch: 31\n",
      "Train accuracy: 459979/469504 (97.971%)\tLoss: 6.739642\n",
      "Test accuracy: 114138/117248 (97.348%)\tLoss: 5.521111\n",
      "Epoch: 32\n",
      "Train accuracy: 460670/469504 (98.118%)\tLoss: 4.978654\n",
      "Test accuracy: 114167/117248 (97.372%)\tLoss: 5.104609\n",
      "Epoch: 33\n",
      "Train accuracy: 460867/469504 (98.160%)\tLoss: 5.504045\n",
      "Test accuracy: 114254/117248 (97.446%)\tLoss: 5.191319\n",
      "Epoch: 34\n",
      "Train accuracy: 461297/469504 (98.252%)\tLoss: 6.221181\n",
      "Test accuracy: 114290/117248 (97.477%)\tLoss: 4.887343\n",
      "Epoch: 35\n",
      "Train accuracy: 461751/469504 (98.349%)\tLoss: 4.989720\n",
      "Test accuracy: 114325/117248 (97.507%)\tLoss: 4.484575\n",
      "Epoch: 36\n",
      "Train accuracy: 462150/469504 (98.434%)\tLoss: 6.110165\n",
      "Test accuracy: 114279/117248 (97.468%)\tLoss: 5.315774\n",
      "Epoch: 37\n",
      "Train accuracy: 462460/469504 (98.500%)\tLoss: 4.856329\n",
      "Test accuracy: 114358/117248 (97.535%)\tLoss: 4.163369\n",
      "Epoch: 38\n",
      "Train accuracy: 462834/469504 (98.579%)\tLoss: 5.549526\n",
      "Test accuracy: 114407/117248 (97.577%)\tLoss: 4.224780\n",
      "Epoch: 39\n",
      "Train accuracy: 462915/469504 (98.597%)\tLoss: 6.608248\n",
      "Test accuracy: 114390/117248 (97.562%)\tLoss: 4.434093\n",
      "Epoch: 40\n",
      "Train accuracy: 463165/469504 (98.650%)\tLoss: 5.792455\n",
      "Test accuracy: 114369/117248 (97.545%)\tLoss: 4.221780\n",
      "Epoch: 41\n",
      "Train accuracy: 463491/469504 (98.719%)\tLoss: 4.590536\n",
      "Test accuracy: 114343/117248 (97.522%)\tLoss: 4.481020\n",
      "Epoch: 42\n",
      "Train accuracy: 463621/469504 (98.747%)\tLoss: 4.368842\n",
      "Test accuracy: 114413/117248 (97.582%)\tLoss: 3.879902\n",
      "Epoch: 43\n",
      "Train accuracy: 463834/469504 (98.792%)\tLoss: 3.769035\n",
      "Test accuracy: 114446/117248 (97.610%)\tLoss: 4.656313\n",
      "Epoch: 44\n",
      "Train accuracy: 464052/469504 (98.839%)\tLoss: 4.908436\n",
      "Test accuracy: 114468/117248 (97.629%)\tLoss: 4.460998\n",
      "Epoch: 45\n",
      "Train accuracy: 464285/469504 (98.888%)\tLoss: 3.967321\n",
      "Test accuracy: 114468/117248 (97.629%)\tLoss: 4.232958\n",
      "Epoch: 46\n",
      "Train accuracy: 464390/469504 (98.911%)\tLoss: 4.457830\n",
      "Test accuracy: 114483/117248 (97.642%)\tLoss: 3.938095\n",
      "Epoch: 47\n",
      "Train accuracy: 464675/469504 (98.971%)\tLoss: 4.733731\n",
      "Test accuracy: 114472/117248 (97.632%)\tLoss: 4.077549\n",
      "Epoch: 48\n",
      "Train accuracy: 464763/469504 (98.990%)\tLoss: 4.506671\n",
      "Test accuracy: 114440/117248 (97.605%)\tLoss: 4.028045\n",
      "Epoch: 49\n",
      "Train accuracy: 464880/469504 (99.015%)\tLoss: 4.257140\n",
      "Test accuracy: 114450/117248 (97.614%)\tLoss: 3.979192\n",
      "Epoch: 50\n",
      "Train accuracy: 465142/469504 (99.071%)\tLoss: 5.275054\n",
      "Test accuracy: 114484/117248 (97.643%)\tLoss: 4.278753\n",
      "Epoch: 51\n",
      "Train accuracy: 465201/469504 (99.084%)\tLoss: 3.725218\n",
      "Test accuracy: 114520/117248 (97.673%)\tLoss: 3.525460\n",
      "Epoch: 52\n",
      "Train accuracy: 465446/469504 (99.136%)\tLoss: 4.137070\n",
      "Test accuracy: 114501/117248 (97.657%)\tLoss: 3.754445\n",
      "Epoch: 53\n",
      "Train accuracy: 465314/469504 (99.108%)\tLoss: 3.371515\n",
      "Test accuracy: 114551/117248 (97.700%)\tLoss: 4.214843\n",
      "Epoch: 54\n",
      "Train accuracy: 465618/469504 (99.172%)\tLoss: 4.267054\n",
      "Test accuracy: 114510/117248 (97.665%)\tLoss: 4.304883\n",
      "Epoch: 55\n",
      "Train accuracy: 465654/469504 (99.180%)\tLoss: 4.126877\n",
      "Test accuracy: 114521/117248 (97.674%)\tLoss: 3.855207\n",
      "Epoch: 56\n",
      "Train accuracy: 465764/469504 (99.203%)\tLoss: 3.450595\n",
      "Test accuracy: 114548/117248 (97.697%)\tLoss: 3.720851\n",
      "Epoch: 57\n",
      "Train accuracy: 465791/469504 (99.209%)\tLoss: 2.919745\n",
      "Test accuracy: 114576/117248 (97.721%)\tLoss: 3.782419\n",
      "Epoch: 58\n",
      "Train accuracy: 465940/469504 (99.241%)\tLoss: 4.019708\n",
      "Test accuracy: 114537/117248 (97.688%)\tLoss: 3.530241\n",
      "Epoch: 59\n",
      "Train accuracy: 465911/469504 (99.235%)\tLoss: 2.915738\n",
      "Test accuracy: 114544/117248 (97.694%)\tLoss: 3.555318\n",
      "Epoch: 60\n",
      "Train accuracy: 466164/469504 (99.289%)\tLoss: 3.083654\n",
      "Test accuracy: 114586/117248 (97.730%)\tLoss: 3.831208\n",
      "Epoch: 61\n",
      "Train accuracy: 466170/469504 (99.290%)\tLoss: 3.837327\n",
      "Test accuracy: 114566/117248 (97.713%)\tLoss: 3.572088\n",
      "Epoch: 62\n",
      "Train accuracy: 466300/469504 (99.318%)\tLoss: 4.209478\n",
      "Test accuracy: 114580/117248 (97.724%)\tLoss: 3.183242\n",
      "Epoch: 63\n",
      "Train accuracy: 466245/469504 (99.306%)\tLoss: 3.205850\n",
      "Test accuracy: 114549/117248 (97.698%)\tLoss: 4.137790\n",
      "Epoch: 64\n",
      "Train accuracy: 466343/469504 (99.327%)\tLoss: 3.751959\n",
      "Test accuracy: 114618/117248 (97.757%)\tLoss: 3.357951\n",
      "Epoch: 65\n",
      "Train accuracy: 466445/469504 (99.348%)\tLoss: 2.456509\n",
      "Test accuracy: 114556/117248 (97.704%)\tLoss: 3.754542\n",
      "Epoch: 66\n",
      "Train accuracy: 466593/469504 (99.380%)\tLoss: 2.566952\n",
      "Test accuracy: 114566/117248 (97.713%)\tLoss: 3.379540\n",
      "Epoch: 67\n",
      "Train accuracy: 466539/469504 (99.368%)\tLoss: 3.564757\n",
      "Test accuracy: 114549/117248 (97.698%)\tLoss: 3.268766\n",
      "Epoch: 68\n",
      "Train accuracy: 466572/469504 (99.376%)\tLoss: 2.458205\n",
      "Test accuracy: 114599/117248 (97.741%)\tLoss: 3.430939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69\n",
      "Train accuracy: 466686/469504 (99.400%)\tLoss: 2.830349\n",
      "Test accuracy: 114556/117248 (97.704%)\tLoss: 3.297306\n",
      "Epoch: 70\n",
      "Train accuracy: 466675/469504 (99.397%)\tLoss: 1.839272\n",
      "Test accuracy: 114619/117248 (97.758%)\tLoss: 3.069195\n",
      "Epoch: 71\n",
      "Train accuracy: 466799/469504 (99.424%)\tLoss: 3.193799\n",
      "Test accuracy: 114601/117248 (97.742%)\tLoss: 3.093598\n",
      "Epoch: 72\n",
      "Train accuracy: 466913/469504 (99.448%)\tLoss: 1.718107\n",
      "Test accuracy: 114581/117248 (97.725%)\tLoss: 3.119102\n",
      "Epoch: 73\n",
      "Train accuracy: 466821/469504 (99.429%)\tLoss: 2.940211\n",
      "Test accuracy: 114607/117248 (97.748%)\tLoss: 3.488417\n",
      "Epoch: 74\n",
      "Train accuracy: 466896/469504 (99.445%)\tLoss: 3.328394\n",
      "Test accuracy: 114586/117248 (97.730%)\tLoss: 3.625930\n",
      "Epoch: 75\n",
      "Train accuracy: 467020/469504 (99.471%)\tLoss: 2.340656\n",
      "Test accuracy: 114585/117248 (97.729%)\tLoss: 3.791187\n",
      "Epoch: 76\n",
      "Train accuracy: 466998/469504 (99.466%)\tLoss: 3.920566\n",
      "Test accuracy: 114584/117248 (97.728%)\tLoss: 3.633781\n",
      "Epoch: 77\n",
      "Train accuracy: 467006/469504 (99.468%)\tLoss: 2.345345\n",
      "Test accuracy: 114611/117248 (97.751%)\tLoss: 3.323125\n",
      "Epoch: 78\n",
      "Train accuracy: 467072/469504 (99.482%)\tLoss: 2.429608\n",
      "Test accuracy: 114551/117248 (97.700%)\tLoss: 3.416608\n",
      "Epoch: 79\n",
      "Train accuracy: 467116/469504 (99.491%)\tLoss: 3.600112\n",
      "Test accuracy: 114582/117248 (97.726%)\tLoss: 3.276882\n",
      "Epoch: 80\n",
      "Train accuracy: 467243/469504 (99.518%)\tLoss: 3.394287\n",
      "Test accuracy: 114597/117248 (97.739%)\tLoss: 3.037668\n",
      "Epoch: 81\n",
      "Train accuracy: 467174/469504 (99.504%)\tLoss: 3.435785\n",
      "Test accuracy: 114636/117248 (97.772%)\tLoss: 3.285191\n",
      "Epoch: 82\n",
      "Train accuracy: 467250/469504 (99.520%)\tLoss: 3.433628\n",
      "Test accuracy: 114571/117248 (97.717%)\tLoss: 3.046356\n",
      "Epoch: 83\n",
      "Train accuracy: 467260/469504 (99.522%)\tLoss: 2.974857\n",
      "Test accuracy: 114611/117248 (97.751%)\tLoss: 3.097574\n",
      "Epoch: 84\n",
      "Train accuracy: 467340/469504 (99.539%)\tLoss: 2.330192\n",
      "Test accuracy: 114624/117248 (97.762%)\tLoss: 3.512966\n",
      "Epoch: 85\n",
      "Train accuracy: 467371/469504 (99.546%)\tLoss: 3.246630\n",
      "Test accuracy: 114597/117248 (97.739%)\tLoss: 3.147444\n",
      "Epoch: 86\n",
      "Train accuracy: 467431/469504 (99.558%)\tLoss: 2.725286\n",
      "Test accuracy: 114602/117248 (97.743%)\tLoss: 3.523417\n",
      "Epoch: 87\n",
      "Train accuracy: 467361/469504 (99.544%)\tLoss: 3.426015\n",
      "Test accuracy: 114636/117248 (97.772%)\tLoss: 3.165701\n",
      "Epoch: 88\n",
      "Train accuracy: 467431/469504 (99.558%)\tLoss: 3.254330\n",
      "Test accuracy: 114634/117248 (97.771%)\tLoss: 3.312305\n",
      "Epoch: 89\n",
      "Train accuracy: 467421/469504 (99.556%)\tLoss: 2.293636\n",
      "Test accuracy: 114588/117248 (97.731%)\tLoss: 3.298170\n",
      "Epoch: 90\n",
      "Train accuracy: 467501/469504 (99.573%)\tLoss: 2.403422\n",
      "Test accuracy: 114619/117248 (97.758%)\tLoss: 3.355186\n",
      "Epoch: 91\n",
      "Train accuracy: 467471/469504 (99.567%)\tLoss: 2.139994\n",
      "Test accuracy: 114599/117248 (97.741%)\tLoss: 3.168272\n",
      "Epoch: 92\n",
      "Train accuracy: 467585/469504 (99.591%)\tLoss: 2.686337\n",
      "Test accuracy: 114598/117248 (97.740%)\tLoss: 3.380862\n",
      "Epoch: 93\n",
      "Train accuracy: 467590/469504 (99.592%)\tLoss: 2.340116\n",
      "Test accuracy: 114610/117248 (97.750%)\tLoss: 3.437028\n",
      "Epoch: 94\n",
      "Train accuracy: 467653/469504 (99.606%)\tLoss: 2.393981\n",
      "Test accuracy: 114548/117248 (97.697%)\tLoss: 3.220236\n",
      "Epoch: 95\n",
      "Train accuracy: 467598/469504 (99.594%)\tLoss: 2.136325\n",
      "Test accuracy: 114590/117248 (97.733%)\tLoss: 3.510327\n",
      "Epoch: 96\n",
      "Train accuracy: 467597/469504 (99.594%)\tLoss: 1.921486\n",
      "Test accuracy: 114611/117248 (97.751%)\tLoss: 3.197449\n",
      "Epoch: 97\n",
      "Train accuracy: 467645/469504 (99.604%)\tLoss: 1.958002\n",
      "Test accuracy: 114572/117248 (97.718%)\tLoss: 3.284661\n",
      "Epoch: 98\n",
      "Train accuracy: 467700/469504 (99.616%)\tLoss: 1.400941\n",
      "Test accuracy: 114587/117248 (97.730%)\tLoss: 3.144646\n",
      "Epoch: 99\n",
      "Train accuracy: 467634/469504 (99.602%)\tLoss: 3.209384\n",
      "Test accuracy: 114598/117248 (97.740%)\tLoss: 3.216543\n",
      "Epoch: 100\n",
      "Train accuracy: 467738/469504 (99.624%)\tLoss: 2.230949\n",
      "Test accuracy: 114592/117248 (97.735%)\tLoss: 3.366622\n",
      "Epoch: 101\n",
      "Train accuracy: 467661/469504 (99.607%)\tLoss: 2.767103\n",
      "Test accuracy: 114595/117248 (97.737%)\tLoss: 2.711042\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-c06a22459ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-4e3e041c83ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, epoch, optimizer)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#with redirect_output(\"deepSNAP_redirect.txt\"):\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "#drop_prob=0.5\n",
    "print(vocab_size)\n",
    "model = Net(vocab_size, output_size=512, embedding_dim=256, hidden_lstm_dim=1024, lstm_layers=1).to(device)\n",
    "# model.linear1_1.weight.requires_grad = False\n",
    "# model.linear1_1.bias.requires_grad = False\n",
    "# model.linear1_2.weight.requires_grad = False\n",
    "# model.linear1_2.bias.requires_grad = False\n",
    "\n",
    "if do_learn: # training mode\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, device, train_loader, epoch, optimizer)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lclhome/mtari008/anaconda3/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'models/lstm_97.7%_v3.0.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove modifications  \n",
    "use one charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([[1., 1.], [1., 2.], [3., 4.]])\n",
    "S = torch.tensor([[2., 3.], [3., 2.], [4., 3.], [5, 3]])\n",
    "\n",
    "mul = torch.mm(T, S.t())\n",
    "print('mul: ')\n",
    "print(mul)\n",
    "adder = torch.tensor([1., 2., 3., 4.])\n",
    "added = adder + mul\n",
    "print('added: ')\n",
    "print(added)\n",
    "\n",
    "norm = T.pow(2).sum(1)\n",
    "print(norm)\n",
    "exp_norm = norm.expand(4, -1).t()\n",
    "print(exp_norm)\n",
    "# pdist = nn.PairwiseDistance(p=2)\n",
    "# output = pdist(T, S)\n",
    "# output\n",
    "# print(T)\n",
    "# print(T.t())\n",
    "# test = torch.tensor([1, 2, 3])\n",
    "# expanded_test = test.expand(3, -1)\n",
    "# print(expanded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(A, B):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = torch.mm(A, B.t())\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    \n",
    "    A_L2_norm = A.pow(2).sum(1)\n",
    "    B_L2_norm = B.pow(2).sum(1)\n",
    "    \n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = A_L2_norm[:, None] - (2.0 * dot_product) + B_L2_norm\n",
    "    \n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = torch.max(distances, torch.tensor(0.0))\n",
    "\n",
    "#     if not squared:\n",
    "#         # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "#         # we need to add a small epsilon where distances == 0.0\n",
    "#         mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "#         distances = distances + mask * 1e-16\n",
    "\n",
    "#         distances = tf.sqrt(distances)\n",
    "\n",
    "#         # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "#         distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = pairwise_distance(T, T)\n",
    "print(T)\n",
    "print(S)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([[1., 1.], [1., 2.], [3., 4.]])  \n",
    "S = torch.tensor([[2., 3.], [3., 2.], [4., 3.], [5, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(x, y=None):\n",
    "    '''\n",
    "    Input: x is a Nxd matrix\n",
    "           y is an optional Mxd matirx\n",
    "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "            if y is not given then use 'y=x'.\n",
    "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "    '''\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    if y is not None:\n",
    "        y_t = torch.transpose(y, 0, 1)\n",
    "        y_norm = (y**2).sum(1).view(1, -1)\n",
    "    else:\n",
    "        y_t = torch.transpose(x, 0, 1)\n",
    "        y_norm = x_norm.view(1, -1)\n",
    "    \n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    # Ensure diagonal is zero if x=y\n",
    "    if y is None:\n",
    "        dist = dist - torch.diag(dist.diag())\n",
    "    dist[dist != dist] = 0 # set all nan values to zero\n",
    "    return torch.clamp(dist, 0.0, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 1.4335, -1.0990, -0.8586],\n",
    "        [ 2.1553,  2.7028, -0.8020],\n",
    "        [ 1.0524,  0.1599, -0.0374]])\n",
    "b = torch.tensor([[ 3., 7., 2.],\n",
    "                [ 6.,  8., 2.],\n",
    "                [ 9.,  5., 7.]])\n",
    "dists = process.pairwise_distances(x=a, y=None)\n",
    "print(dists)\n",
    "print(b.max(1).values)\n",
    "print(b[b.max(1).indices])\n",
    "#print(b.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 3., 1., 4.],\n",
    "                [ 0.,  2., 3.],\n",
    "                [ 5.,  3., 6.]])\n",
    "b = torch.tensor([[ 1., 2., 1.],\n",
    "                [ 2.,  2., 1.],\n",
    "                [ 1.,  1., 1.]])\n",
    "dist = (a - b) ** 2\n",
    "print(l2_squared(a, b))\n",
    "print(torch.sum(l2_squared(a, b), 1))\n",
    "print(a.min(1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = torch.tensor([1., -1., 3.])\n",
    "max0 = torch.max(a0, torch.tensor(0.))\n",
    "print(max0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([1, 2, 3, 4])\n",
    "b = np.asarray([0, 1, 0])\n",
    "a.append(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_print(i):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_path = join(in_tensor_dir, 'peptides')\n",
    "charge1 = 0\n",
    "charge2 = 0\n",
    "charge3 = 0\n",
    "for file in listdir(pep_path):\n",
    "    file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pep\", file)\n",
    "    charge = int(file_parts[4])\n",
    "    if charge == 1:\n",
    "        charge1 += 1\n",
    "    elif charge == 2:\n",
    "        charge2 += 1\n",
    "    elif charge == 3:\n",
    "        charge3 += 1\n",
    "print('charge 1 count: {}'.format(charge1))\n",
    "print('charge 2 count: {}'.format(charge2))\n",
    "print('charge 3 count: {}'.format(charge3))\n",
    "\n",
    "unmod = 0\n",
    "mod = 0\n",
    "for file in listdir(pep_path):\n",
    "    file_parts = re.search(r\"(\\d+)-(\\d+)-(\\d+.\\d+)-(\\d)-(0|1).pep\", file)\n",
    "    m = int(file_parts[5])\n",
    "    if m == 0:\n",
    "        unmod += 1\n",
    "    elif m == 1:\n",
    "        mod += 1\n",
    "print('Unmodified count: {}'.format(unmod))\n",
    "print('Modified count: {}'.format(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Name: AAAAAAAAAAAAAAAGAGAGAK/2_0\"\n",
    "name_groups = re.search(r\"Name:\\s(?P<pep>[a-zA-Z]+)/(?P<charge>\\d+)\"\n",
    "                                    r\"(?:_(?P<num_mods>\\d+)(?P<mods>.*))?\", line)\n",
    "pep = name_groups['pep']\n",
    "l_charge = int(name_groups['charge'])\n",
    "modified = (name_groups['num_mods']) != '0'\n",
    "\n",
    "print(pep)\n",
    "print(l_charge)\n",
    "print(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(round(1.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
