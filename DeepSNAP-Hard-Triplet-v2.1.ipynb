{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import queue\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output\n",
    "import csv\n",
    "from heapq import merge\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "import ast\n",
    "\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "rand.seed(37)\n",
    "\n",
    "#from src.snapconfig import config\n",
    "from src.snapprocess import simulatespectra as sim\n",
    "from src.snapprocess import process\n",
    "from src.snaputils import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary config func. Original one in the project.\n",
    "class config:\n",
    "    \"\"\"Define constants\"\"\"\n",
    "    AAMass = {'A': 71.037114, 'C': 103.009185, 'D': 115.026943, 'E': 129.042593, 'F': 147.068414, 'G': 57.021464,\n",
    "              'H': 137.058912, 'I': 113.084064, 'K': 128.094963, 'L': 113.084064, 'M': 131.040485, 'N': 114.042927,\n",
    "              'P': 97.052764, 'Q': 128.058578, 'R': 156.101111, 'S': 87.032028, 'T': 101.047679, 'V': 99.068414,\n",
    "              'W': 186.079313, 'Y': 163.0633}\n",
    "\n",
    "    H2O = 18.015\n",
    "    NH3 = 17.031\n",
    "    PROTON = 1.00727647\n",
    "    DEFAULT_PARAM_PATH = os.path.join(os.getcwd(), 'config.ini')\n",
    "    PARAM_PATH = None\n",
    "    l_config = None\n",
    "\n",
    "\n",
    "    def get_config(section='input', key=None):\n",
    "        \"\"\"Read the configuration parameters and return a dictionary.\"\"\"\n",
    "\n",
    "        # If file path is given use it otherwise use default.\n",
    "        file_path = config.PARAM_PATH if config.PARAM_PATH else config.DEFAULT_PARAM_PATH\n",
    "\n",
    "        # Read config and convert each value to appropriate type.\n",
    "        # Only for the first time.\n",
    "        if not config.l_config:\n",
    "            config.l_config = dict()\n",
    "            config_ = ConfigParser()\n",
    "            assert isinstance(file_path, str)\n",
    "            config_.read(file_path)\n",
    "            for section_ in config_.sections():\n",
    "                config.l_config[section_] = dict()\n",
    "                for key_ in config_[section_]:\n",
    "                    try:\n",
    "                        config.l_config[section_][key_] = ast.literal_eval(config_[section_][key_])\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        config.l_config[section_][key_] = config_[section_][key_]\n",
    "\n",
    "        if section and section in config.l_config:\n",
    "            if key and key in config.l_config[section]:\n",
    "                return config.l_config[section][key]\n",
    "            return config.l_config[section]\n",
    "        return config.l_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# AAMass = {'A':71.037114, 'C':103.009185, 'D':115.026943, 'E':129.042593, 'F':147.068414, 'G':57.021464, 'H':137.058912,\n",
    "#           'I':113.084064, 'K':128.094963, 'L':113.084064, 'M':131.040485, 'N':114.042927, 'P':97.052764, 'Q':128.058578,\n",
    "#           'R':156.101111, 'S':87.032028, 'T':101.047679, 'V':99.068414, 'W':186.079313, 'Y':163.0633}\n",
    "\n",
    "# H2O = 18.015\n",
    "# NH3 = 17.031\n",
    "# PROTON = 1.00727647\n",
    "# specsize = 8000\n",
    "# charge = 2\n",
    "# use_mods = False\n",
    "\n",
    "# def GetAAMass(AA):\n",
    "#     return AAMass[AA] + 57.021464 if AA == 'C' else AAMass[AA]\n",
    "\n",
    "# def GetSpectrum(Seq):\n",
    "#     size = len(Seq)\n",
    "#     outsize = 2*size\n",
    "#     bspectrum = []\n",
    "#     bspectrumaux = []\n",
    "#     yspectrum = []\n",
    "#     yspectrumaux = []\n",
    "    \n",
    "#     bspectrum.append(GetAAMass(Seq[0]) + PROTON)\n",
    "#     yspectrum.append(GetAAMass(Seq[-1]) + H2O + PROTON)\n",
    "    \n",
    "#     for i, (fAA, bAA) in enumerate(zip((Seq[1:]), Seq[-2::-1])):\n",
    "#         bspectrum.append(bspectrum[i] + GetAAMass(fAA))\n",
    "#         yspectrum.append(yspectrum[i] + GetAAMass(bAA))\n",
    "    \n",
    "# #    bspectrumaux = [(bion + PROTON) / 2 for bion in bspectrum]\n",
    "# #    bspectrumaux.extend([bion - H2O for bion in bspectrum])\n",
    "# #    bspectrumaux.extend([bion - NH3 for bion in bspectrum])\n",
    "    \n",
    "# #    yspectrumaux = [(yion + PROTON) / 2 for yion in yspectrum]\n",
    "# #    yspectrumaux.extend([yion - H2O for yion in yspectrum])\n",
    "# #    yspectrumaux.extend([yion - NH3 for yion in yspectrum])\n",
    "    \n",
    "# #    bspectrum.extend(bspectrumaux)\n",
    "# #    yspectrum.extend(yspectrumaux)\n",
    "    \n",
    "# #     bspectrum.sort()\n",
    "# #     yspectrum.sort()\n",
    "    \n",
    "#     mergedout = list(merge(bspectrum, yspectrum))\n",
    "#     if mergedout[-1] > specsize:\n",
    "#         print(mergedout[-1])\n",
    "#         print(Seq)\n",
    "#     tspec = np.zeros(specsize)\n",
    "#     tspec[np.rint(mergedout).astype(int)] = 1\n",
    "#     return tspec\n",
    "\n",
    "# by = GetSpectrum('ACDEFG')\n",
    "# print('printing by:\\n' + str(by))\n",
    "# print(by.shape)\n",
    "# print(AAMass['L'])\n",
    "\n",
    "#### Sin Mods:\n",
    "# Number of charge 1 examples: 18230  \n",
    "# Number of charge 2 examples: 130172  \n",
    "# Number of charge 3 examples: 70741  \n",
    "  \n",
    "#### Con Mods:\n",
    "# Number of charge 2 examples: 184994\n",
    "\n",
    "# def getrandmod(seq, nummods=1):\n",
    "#     AAs = list(AAMass.keys())\n",
    "#     res = temp = seq\n",
    "#     for i in range(nummods):\n",
    "#         while res == temp:\n",
    "#             randindx = rand.randint(0, len(seq)-1)\n",
    "#             randmod = AAs[rand.randint(0, len(AAs))-1]\n",
    "#             temp = temp[:randindx] + randmod + temp[randindx+1:]\n",
    "#         res = temp\n",
    "#     return res\n",
    "\n",
    "# seq = 'AFINSTWDG'\n",
    "# print(getrandmod(seq))\n",
    "\n",
    "# def readmspwithdecoy(mspfile):\n",
    "#     f=open(mspfile, \"r\")\n",
    "#     lines = f.readlines()\n",
    "#     f.close()\n",
    "#     #fo = open('output.csv', 'w')\n",
    "#     #fo.write('Q,P,N\\n')\n",
    "#     fixedlen = 300\n",
    "#     dataset = []\n",
    "#     label = []\n",
    "#     print('len of file: ' + str(len(lines)))\n",
    "#     count = 0\n",
    "#     limit = 200000\n",
    "#     pep = mass = numPeaks = 0\n",
    "#     #spec = []\n",
    "#     isName = isMW = isNumPeaks = False\n",
    "#     new = prev = 0\n",
    "#     maxpeaks = maxmoz = 0\n",
    "#     i = 0\n",
    "#     while i < len(lines) and limit > 0:\n",
    "#         line = lines[i]\n",
    "#         i += 1\n",
    "#         splits = line.split(':') \n",
    "#         if (splits[0] == 'Name') and '_' in line:\n",
    "#             split1 = splits[1]\n",
    "#             l_charge = int(split1[split1.find('_') - 1])\n",
    "#             if l_charge != charge: # l_charge == l_charge always true.\n",
    "#                 continue\n",
    "#             if use_mods:\n",
    "#                 pep = split1.split('/')[0].lstrip(' ')\n",
    "#                 isName = True\n",
    "#             elif '(' not in splits[1] and ')' not in splits[1]:\n",
    "#                 pep = split1.split('/')[0].lstrip(' ')\n",
    "#                 isName = True\n",
    "\n",
    "#         if (isName and splits[0] == 'MW'):\n",
    "#             mass = float(splits[1])\n",
    "#             if round(mass) < specsize:\n",
    "#                 isMW = True\n",
    "#                 #limit = limit - 1\n",
    "#             else:\n",
    "#                 isName = isMW = isNumPeaks = False\n",
    "#                 continue\n",
    "\n",
    "#         if (isName and isMW and splits[0] == 'Num peaks'):\n",
    "#             numPeaks = int(splits[1])\n",
    "#             if numPeaks > maxpeaks:\n",
    "#                 maxpeaks = numPeaks\n",
    "\n",
    "#             spec = np.zeros(specsize)\n",
    "#             while (lines[i] != '\\n'):\n",
    "#                 mzline = lines[i]\n",
    "#                 i +=1\n",
    "#                 mzsplits = mzline.split('\\t')\n",
    "#                 moz, intensity = float(mzsplits[0]), float(mzsplits[1])\n",
    "#                 if moz > maxmoz:\n",
    "#                     maxmoz = moz\n",
    "#                 spec[round(moz)] += round(intensity)\n",
    "\n",
    "#             spec = np.clip(spec, None, 1000.0)\n",
    "#             spec = preprocessing.scale(spec)\n",
    "\n",
    "#             isNumPeaks = True\n",
    "\n",
    "#         if isName and isMW and isNumPeaks:\n",
    "#             isName = isMW = isNumPeaks = False\n",
    "#             #revPep = pep[0] + pep[1:-1][::-1] + pep[-1]\n",
    "#             revPep = getrandmod(pep)\n",
    "#             if pep == revPep:\n",
    "#                 print('decoy is the same. shuffling')\n",
    "#                 #revPep = ''.join(rand.sample(revPep,len(revPep)))\n",
    "#                 revPep = getrandmod(pep, len(pep))\n",
    "#                 print(pep)\n",
    "#                 print(revPep)\n",
    "#             tspec = preprocessing.scale(GetSpectrum(pep))\n",
    "#             rtspec = preprocessing.scale(GetSpectrum(revPep))\n",
    "\n",
    "#             dataset.append([spec, tspec, rtspec])\n",
    "#             label.append([1, -1])\n",
    "\n",
    "#             count = count + 1\n",
    "#             pep = mass = numPeaks = 0\n",
    "#             spec = []\n",
    "#             new = int((i/len(lines)) * 100)\n",
    "#             if (new > prev):\n",
    "#                 #clear_output(wait=True)\n",
    "#                 print(str(new) + '%')\n",
    "#                 prev = new\n",
    "    \n",
    "#     print('max peaks: ' + str(maxpeaks))\n",
    "#     print('count: ' + str(count))\n",
    "#     print('max moz: ' + str(maxmoz))\n",
    "#     return dataset, label\n",
    "# # print('max peaks: ' + str(maxpeaks))\n",
    "# # print('count: ' + str(count))\n",
    "# # print('max moz: ' + str(maxmoz))\n",
    "# # #fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/msp/human_consensus_final_true_lib.msp\", \"r\")\n",
    "lines = f.readlines()\n",
    "newcontents = []\n",
    "specid = 0\n",
    "prev = 0\n",
    "num_specs = 0\n",
    "ll_charge = config.get_config(section='input', key='charge')\n",
    "using_mods = config.get_config(section='input', key='use_mods')\n",
    "for i, line in enumerate(lines):\n",
    "    if line.startswith('Name') and '_' in line:\n",
    "        l_charge = int(line[line.find('_') - 1])\n",
    "        if ll_charge > 0 and l_charge != ll_charge:\n",
    "            continue\n",
    "        if using_mods:\n",
    "            num_specs += 1\n",
    "        elif '(' not in line and ')' not in line:\n",
    "            num_specs += 1\n",
    "        new = int((i/len(lines)) * 100)\n",
    "        if (new > prev):\n",
    "            clear_output(wait=True)\n",
    "            print(str(new) + '%')\n",
    "            prev = new\n",
    "f.close()\n",
    "print(type(lines))\n",
    "print('len of file: ' + str(len(lines)))\n",
    "print(num_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.get_config(section='input', key='charge'))\n",
    "print(config.get_config(section='input', key='use_mods'))\n",
    "print(config.get_config(section='ml', key='batch_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pep_list, dataset, label = reader.read_msp(\"data/msp/human_consensus_final_true_lib.msp\", decoy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(pep_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('splitting...')\n",
    "tmpTrainData, tmpTestData = train_test_split(\n",
    "            dataset, test_size = 0.2, random_state = rand.randint(0, 1000), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_peps = set([item[0] for item in tmpTrainData])\n",
    "# test_peps = set([item[0] for item in tmpTestData])\n",
    "# print(sorted(list(train_peps))[0:10])\n",
    "# print(sorted(list(test_peps))[0:10])\n",
    "# print(len(train_peps))\n",
    "# print(len(test_peps))\n",
    "# common = train_peps.intersection(test_peps)\n",
    "# print(len(common))\n",
    "# print(len(tmpTestData))\n",
    "# for item in tmpTestData:\n",
    "#     if item[0] in common:\n",
    "#         tmpTestData.remove(item)\n",
    "        \n",
    "# print(len(tmpTestData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('converting to tensors...')\n",
    "XTrainTensor = torch.tensor(tmpTrainData, dtype=torch.float)\n",
    "XTestTensor = torch.tensor(tmpTestData, dtype=torch.float)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset, tmpTrainData, tmpTestData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=XTrainTensor, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=XTestTensor, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_learn = True\n",
    "save_frequency = 2\n",
    "lr = 0.00001\n",
    "num_epochs = 200\n",
    "weight_decay = 0.001\n",
    "margin = 0.2\n",
    "#torch.manual_seed(0)\n",
    "#torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = torch.load('train_loader.pt')\n",
    "#test_loader = torch.load('test_loader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.spec_size = config.get_config(section='input', key='spec_size')\n",
    "        self.searching = False\n",
    "        \n",
    "        self.linear1_1 = nn.Linear(self.spec_size, 1024)\n",
    "        self.linear1_2 = nn.Linear(1024, 512)\n",
    "        self.linear1_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.linear2_1 = nn.Linear(self.spec_size, 1024)\n",
    "        self.linear2_2 = nn.Linear(1024, 512)\n",
    "        self.linear2_3 = nn.Linear(512, 256)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        #self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        res = []\n",
    "#         x = data[:, 0]\n",
    "#         x = self.linear1_1(x.view(-1, self.spec_size))\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "        #x = self.linear1_2(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.linear1_3(x)\n",
    "        #x = F.relu(x)\n",
    "        #if not self.searching:\n",
    "        #    x = F.normalize(x)\n",
    "        #res.append(x)\n",
    "        for i in range(data.shape[1]):\n",
    "            x = data[:, i]\n",
    "            x = self.linear2_1(x.view(-1, self.spec_size))\n",
    "            x = F.relu(x)\n",
    "            #x = torch.tanh(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.linear2_2(x)\n",
    "            x = F.relu(x)\n",
    "            #x = torch.tanh(x)\n",
    "            #x = self.dropout2(x)\n",
    "            #x = self.dropout1(x)\n",
    "            #x = self.linear2_3(x)\n",
    "            #x = F.relu(x)\n",
    "            #if not self.searching:\n",
    "            x = F.normalize(x)\n",
    "            #x = self.linear1_3(x)\n",
    "            #x = F.relu(x)\n",
    "            res.append(x)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (tid, data) in enumerate(train_loader):\n",
    "#     if tid == 0:\n",
    "#         print(data[:,0:3:2].shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hinge = torch.nn.HingeEmbeddingLoss()\n",
    "triplet_loss = nn.TripletMarginLoss(margin=margin, p=2, reduction='mean')\n",
    "l2_squared = nn.MSELoss(reduction='none')\n",
    "pdist = nn.PairwiseDistance(p=2)\n",
    "zero_tensor = torch.tensor(0.).to(device)\n",
    "\n",
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    accurate_labels = 0\n",
    "    all_labels = 0\n",
    "    for (batch_idx, data) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)    \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        Q, P = model(data)\n",
    "        \n",
    "        \"\"\"Mine the hardest triplets. Get rid of N.\"\"\" \n",
    "        QxQ = process.pairwise_distances(Q)    # calculate distance matrix for spectra\n",
    "        PxP = process.pairwise_distances(P)    # calculate distance matrix for peptides\n",
    "        QxP_ = process.pairwise_distances(Q, P) # calculate distance matrix for spectra-peptides\n",
    "        \n",
    "        # Set the diagonal of all distance matrices to inf so we don't get self as the closest negative.\n",
    "        QxQ.fill_diagonal_(float(\"inf\"))\n",
    "        PxP.fill_diagonal_(float(\"inf\"))\n",
    "        QxP = QxP_.clone()\n",
    "        QxP.fill_diagonal_(float(\"inf\"))\n",
    "        \n",
    "        #print(QP.argmin(1)[:100])\n",
    "        \n",
    "        pos = 4 * (torch.sum(l2_squared(Q, P), dim=1) + margin)\n",
    "        \n",
    "        QxQ_min = QxQ.min(1).values              # farthest spectrum for each spectrum\n",
    "        PxP_min = PxP.min(1).values              # farthest peptide for each peptide\n",
    "        QxP_min = QxP.min(1).values              # farthest peptide for each spectrum\n",
    "        PxQ_min = QxP.min(0).values              # farthest spectrum for each peptide\n",
    "        \n",
    "        neg = QxQ_min + PxP_min + QxP_min + PxQ_min\n",
    "        \n",
    "        divider = torch.tensor(float(len(pos)))\n",
    "        #divider = torch.sum(pos - neg > 0)\n",
    "        loss = torch.sum(torch.max(pos - neg, zero_tensor)) / divider\n",
    "        \n",
    "#         loss = triplet_loss(Q, P, QQ_max)     # spectrum-spectrum negatives\n",
    "#         loss += triplet_loss(Q, P, QP_max)    # spectrum-peptide negatives\n",
    "#         loss += triplet_loss(P, Q, PP_max)    # peptide-peptide negatives\n",
    "#         loss += triplet_loss(P, Q, PQ_max)    # peptide-spectrum negatives\n",
    "        \n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        \n",
    "        seq = torch.arange(0, len(Q), step=1, device=device, requires_grad=False)\n",
    "        accurate_labels = accurate_labels + torch.sum(QxP_.argmin(1) == seq) # use QP_ since it doesn't have diag set to zero\n",
    "        \n",
    "        all_labels = all_labels + len(Q)  \n",
    "    \n",
    "    accuracy = 100. * float(accurate_labels) / all_labels\n",
    "    train_accuracy.append(accuracy)\n",
    "    train_loss.append(loss)\n",
    "    print('Epoch: ' + str(epoch))\n",
    "    print('Train accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "    \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        accurate_labels = 0\n",
    "        all_labels = 0\n",
    "        loss = 0\n",
    "        \n",
    "        for (batch_idx, data) in enumerate(test_loader):\n",
    "            \n",
    "            data = data.to(device)    \n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Q, P = model(data)\n",
    "            \n",
    "            \"\"\"Mine the hardest triplets. Get rid of N.\"\"\" \n",
    "            QxQ = process.pairwise_distances(Q)    # calculate distance matrix for spectra\n",
    "            PxP = process.pairwise_distances(P)    # calculate distance matrix for peptides\n",
    "            QxP_ = process.pairwise_distances(Q, P) # calculate distance matrix for spectra-peptides\n",
    "\n",
    "            # Set the diagonal of all distance matrices to inf so we don't get self as the closest negative.\n",
    "            QxQ.fill_diagonal_(float(\"inf\"))\n",
    "            PxP.fill_diagonal_(float(\"inf\"))\n",
    "            QxP = QxP_.clone()    # clone to measure accuracy. can be done in a better way.\n",
    "            QxP.fill_diagonal_(float(\"inf\"))\n",
    "\n",
    "            #print(QP.argmin(1)[:100])\n",
    "\n",
    "            pos = 4 * (torch.sum(l2_squared(Q, P), dim=1) + margin)\n",
    "\n",
    "            QxQ_min = QxQ.min(1).values              # farthest spectrum for each spectrum\n",
    "            PxP_min = PxP.min(1).values              # farthest peptide for each peptide\n",
    "            QxP_min = QxP.min(1).values              # farthest peptide for each spectrum\n",
    "            PxQ_min = QxP.min(0).values              # farthest spectrum for each peptide\n",
    "\n",
    "            neg = QxQ_min + PxP_min + QxP_min + PxQ_min\n",
    "\n",
    "            divider = torch.tensor(float(len(pos)))\n",
    "            #divider = torch.sum(pos - neg > 0)\n",
    "            loss = torch.sum(torch.max(pos - neg, zero_tensor)) / divider\n",
    "            \n",
    "#             loss =  torch.sum(torch.max(pos - QxQ_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - QxP_min, zero_tensor)) / divider\n",
    "#             loss += torch.sum(torch.max(pos - PxQ_min, zero_tensor)) / divider\n",
    "            \n",
    "            seq = torch.arange(0, len(Q), step=1, device=device, requires_grad=False)\n",
    "            accurate_labels = accurate_labels + torch.sum(QxP_.argmin(1) == seq) # use QP_ since it doesn't have diag set to zero\n",
    "            \n",
    "            all_labels = all_labels + len(Q)\n",
    "                \n",
    "        accuracy = 100. * float(accurate_labels) / all_labels\n",
    "        test_accuracy.append(accuracy)\n",
    "        test_loss.append(loss)\n",
    "        print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "        output = model(data)\n",
    "        return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "model = Net().to(device)\n",
    "# model.linear1_1.weight.requires_grad = False\n",
    "# model.linear1_1.bias.requires_grad = False\n",
    "# model.linear1_2.weight.requires_grad = False\n",
    "# model.linear1_2.bias.requires_grad = False\n",
    "   \n",
    "if do_learn: # training mode\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, device, train_loader, epoch, optimizer)\n",
    "        test(model, device, test_loader)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'models/custome_triplet_loss_93.6_charged%.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove modifications  \n",
    "use one charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([[1., 1.], [1., 2.], [3., 4.]])\n",
    "S = torch.tensor([[2., 3.], [3., 2.], [4., 3.], [5, 3]])\n",
    "\n",
    "mul = torch.mm(T, S.t())\n",
    "print('mul: ')\n",
    "print(mul)\n",
    "adder = torch.tensor([1., 2., 3., 4.])\n",
    "added = adder + mul\n",
    "print('added: ')\n",
    "print(added)\n",
    "\n",
    "norm = T.pow(2).sum(1)\n",
    "print(norm)\n",
    "exp_norm = norm.expand(4, -1).t()\n",
    "print(exp_norm)\n",
    "# pdist = nn.PairwiseDistance(p=2)\n",
    "# output = pdist(T, S)\n",
    "# output\n",
    "# print(T)\n",
    "# print(T.t())\n",
    "# test = torch.tensor([1, 2, 3])\n",
    "# expanded_test = test.expand(3, -1)\n",
    "# print(expanded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(A, B):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = torch.mm(A, B.t())\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    \n",
    "    A_L2_norm = A.pow(2).sum(1)\n",
    "    B_L2_norm = B.pow(2).sum(1)\n",
    "    \n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = A_L2_norm[:, None] - (2.0 * dot_product) + B_L2_norm\n",
    "    \n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = torch.max(distances, torch.tensor(0.0))\n",
    "\n",
    "#     if not squared:\n",
    "#         # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "#         # we need to add a small epsilon where distances == 0.0\n",
    "#         mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "#         distances = distances + mask * 1e-16\n",
    "\n",
    "#         distances = tf.sqrt(distances)\n",
    "\n",
    "#         # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "#         distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = pairwise_distance(T, T)\n",
    "print(T)\n",
    "print(S)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.tensor([[1., 1.], [1., 2.], [3., 4.]])  \n",
    "S = torch.tensor([[2., 3.], [3., 2.], [4., 3.], [5, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(x, y=None):\n",
    "    '''\n",
    "    Input: x is a Nxd matrix\n",
    "           y is an optional Mxd matirx\n",
    "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
    "            if y is not given then use 'y=x'.\n",
    "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
    "    '''\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    if y is not None:\n",
    "        y_t = torch.transpose(y, 0, 1)\n",
    "        y_norm = (y**2).sum(1).view(1, -1)\n",
    "    else:\n",
    "        y_t = torch.transpose(x, 0, 1)\n",
    "        y_norm = x_norm.view(1, -1)\n",
    "    \n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    # Ensure diagonal is zero if x=y\n",
    "    if y is None:\n",
    "        dist = dist - torch.diag(dist.diag())\n",
    "    dist[dist != dist] = 0 # set all nan values to zero\n",
    "    return torch.clamp(dist, 0.0, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 1.4335, -1.0990, -0.8586],\n",
    "        [ 2.1553,  2.7028, -0.8020],\n",
    "        [ 1.0524,  0.1599, -0.0374]])\n",
    "b = torch.tensor([[ 3., 7., 2.],\n",
    "                [ 6.,  8., 2.],\n",
    "                [ 9.,  5., 7.]])\n",
    "dists = process.pairwise_distances(x=a, y=None)\n",
    "print(dists)\n",
    "print(b.max(1).values)\n",
    "print(b[b.max(1).indices])\n",
    "#print(b.max(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 3., 1., 4.],\n",
    "                [ 0.,  2., 3.],\n",
    "                [ 5.,  3., 6.]])\n",
    "b = torch.tensor([[ 1., 2., 1.],\n",
    "                [ 2.,  2., 1.],\n",
    "                [ 1.,  1., 1.]])\n",
    "dist = (a - b) ** 2\n",
    "print(l2_squared(a, b))\n",
    "print(torch.sum(l2_squared(a, b), 1))\n",
    "print(a.min(1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = torch.tensor([1., -1., 3.])\n",
    "max0 = torch.max(a0, torch.tensor(0.))\n",
    "print(max0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([1, 2, 3, 4])\n",
    "b = np.asarray([0, 1, 0])\n",
    "a.append(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip = 'abcde'\n",
    "c = 2\n",
    "r = pip + str(c)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.randint(100, (10,10))\n",
    "print(m)\n",
    "ids = torch.randint(10, (10,)).long()\n",
    "print(ids)\n",
    "print(m.gather(1, ids.view(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
